\section{Application-Specific Functions}\label{app_func}
%III.	Application Functions
%	a.	Resource Value Functions
%		i.	Structure
%		ii.	Model Building Approach
%			1.	Who measures the data?
%	b.	Penalty Functions
%		i.	Where do the deadlines come from?
%		ii.	How do we set the slopes?
%	c.	Power Model


In this section, we describe the design of \pacora's response time and penalty functions in more detail.

\subsection*{Response Time Functions}
A response time function describes an application's performance given its resource assignments.  These functions capture information about the value of a particular resource to an application on the current hardware at a particular time. Without this information, it would be difficult for any resource allocation system to make informed decisions short of blindly trying a variety of allocations and picking the best one.

\subsubsection*{Model Selection} While it might have been possible to model response times by recording past values and interpolating among them, this idea has serious shortcomings:
\begin{itemize}
\item The multidimensional response time tables would be large;
\item Interpolation in many dimensions is computationally expensive;
\item The measurements will be noisy and require smoothing;
\item Convexity in the resources may be violated;
\item Gradient estimation will be slow and difficult.
\end{itemize}

Instead of interpolating, \pacora maintains a parameterized analytic response time model with the partial derivatives evaluated from the model \emph{a priori}. Application responsiveness is highly nonlinear for an increasing variety of applications like streaming media or gaming, thus requiring many data points to represent the response times without a model. Using models, each application can be described in a small number of parameters.  Models can be built from just a few data points and can naturally smooth out noisy data. Their gradients, needed by \pacora to solve the optimization problem efficiently, are easy to calculate.

\pacora models response times with functions that are convex by construction.  The specific function chosen for \pacora is shown in Equation~\ref{rtf_eq} above.  In this equation, the response time is modeled as a weighted sum of component terms, one per resource, where each term $w_r/a_r$ is the amount of work $w_r \geq 0$ divided by $a_r$, the allocation of that resource\cite{Snav}. For example, one term might model the number of instructions executed divided by total processor MIPS, another might model storage accesses divided by storage allocation, and so forth. Asynchrony and latency tolerance may make response time components overlap partly or fully; and thus we added additional terms to represent the interactions between resources. We choose this specific function because in initial application studies we found it models response time behavior accurately enough to allow the optimization to make good decisions and is inexpensive to build.  Alternative models and the initial model evaluations are described in Appendix~\ref{rtf_choice}.

Such models are automatically convex in the allocations because $1/a$ is convex for positive $a$ and because a positively-weighted sum of convex functions is convex.  It is obviously important to guarantee the positivity of the resource allocations. This guarantee can be enforced as the allocations are selected during penalty optimization, or the response time model can be made to return $\infty$ if any allocation is less than or equal to zero. This latter idea preserves the convexity of the model and extends its domain to all of $\Re^n$ and consequently we used this approach in our implementation. The gradient $\nabla\tau$ is needed by the penalty optimization algorithm.
Since $\tau$ is analytic, generic, and symbolically differentiable
it is a simple matter to compute the gradient of $\tau$ once the model is defined.

\subsubsection*{Convexity Assumption} Forcing our response time functions to be convex assumes that the actual response time are reasonably convex. We find this to be a plausible requirement as applications almost completely follow the proverbial ``Law of Diminishing Returns'' for resource allocations.

However, there are examples of response time versus resource behavior that violate convexity.   For example, we have seen non-convex performance in applications when dealing with hyperthreads or memory pages.  For two of our applications 5 hyperthreads resulted in significantly worse performance than 4 or 6 hyperthreads.  When studying some other applications applications we found that particular numbers of memory pages, \emph{e.g.} 2K, resulted in much better performance than the adjacent page allocations.  Avoiding or seeking these allocations adds significant cost to the optimization problem.

Another kind of convexity violation can occur in memory allocation, where ``plateaus'' can sometimes be seen as in Figure~\ref{f:plat}. Such plateaus are typically caused by algorithm adaptations within the application to accommodate variable resource availability.  The response time is really the \emph{minimum} of several convex functions depending on allocation and the point-wise minimum that the application implements fails to preserve convexity.  The effect of the plateaus will be a non-convex penalty as shown in Figure~\ref{f:plateffect} and multiple extrema in the optimization problem will be a likely result. There are several ways to avoid this problem.  One is based on the observation that such response time functions will at least be \emph{quasiconvex}.  Another idea is to use additional constraints to explore convex sub-domains of $\tau$. Either approach adds significant computational cost.

\begin{figure}[hb]
\parbox{1.6in}{
\includegraphics*{Plateau1.eps}
\caption{\label{f:plat}Response time function with some resource ``plateaus''.}
}
\hspace{\fill}
\parbox{1.6in}{
\includegraphics*{Plateau2.eps}
\caption{\label{f:plateffect}Net effect of the resource plateaus on the application penalty.}
}
\end{figure}

Overall, we found that our simple convex models still resulted in high-quality resource allocations and thus chose not to implement any of these approaches.  Alternative approaches to handling non-convex behavior are described in Appendix~\ref{convex}. Additional challenges to response time modeling are discussed in Section~\ref{discuss}.

%\subsubsection*{Dynamically Changing Applications}
%
%
%\fix{changing apps}
%\fix{dynamic compilation}
%\fix{phases}
%Response time will commonly vary with time as a application changes phase and makes better or worse use of its resources.
%\fix{non-determinism}
%
%\fix{Input Variability}


\subsubsection*{Data Collection and Creation Time}
There are many ways to collect the response time data for applications. A user-level runtime scheduler that schedules work internal to the application may be a good source of data, or the operating system could measure progress using performance counters.  In our implementation, applications report their own measured values: however, this solution was chosen simply as a way to test the validity of the concept.  In a production operating system, it may not be the best approach because it requires trusting applications not to lie about their performance.  In a datacenter environment this may be less of a concern.

There are also many different possible moments response time functions could be created.  RTFs could be created in advance and distributed with the application. In the case of app stores, this approach could make lot of sense since most app stores only cater to a limited number of platforms. Data could also be crowd sourced and the RTFs built in the cloud, which has the advantage making it easy to collect a diverse set of training points.  However, all of these approaches are missing personalization.  As a result, we have chosen to implement two solutions that collect data directly from the user's machine.  The first approach is to collect all of the training points at application install time and build the model then.  The more advanced approached collects data continuously as the application runs and adds that data to the model training set and rebuilds the model.  Although ultimately a hybrid approach may be the most effective: applications can begin with a provided generic model, and the system can improve the model over time. Section~\ref{model_creation} describes our model creation process in detail.

\subsection*{Penalty Functions}
Penalty functions are generically defined as members of a family of such functions
so that user preferences for an application $p$ can implemented by assigning values to a few well-understood parameters.
We designed \pacora's penalty functions to represent deadlines and the relative importance of making them.
These values are represented by two parameters a deadline $d$ and a slope $s$.  Equation~\ref{pen_eq} shows penalty functions used in \pacora.

A response-time constrained application has a marked change in slope, namely from 0 to $s$, at the point $\tau= d$. In the most extreme case $s = \infty$ (implying infinite penalty for the system as a whole when $\tau > d$).  For applications without response time constraints one can set $d = 0$. This defines linear behavior with $s$ as the rate of penalty increase with response time.

To guarantee the penalty is convex and non-decreasing, $s$ must be non-negative.
The response time $\tau$  is of course non-negative, and it may be sensible (if not strictly necessary) to require that $d$ is also.

The gradient of application penalty with respect to its resource allocations is used in the optimization algorithm.
By the chain rule, $\partial\pi/\partial a_r = \partial\pi/\partial\tau\cdot\partial\tau/\partial a_r$.
The first term is well-defined, albeit discontinuous at $\tau = d$ with
$\partial\pi/\partial\tau = \mbox{ if } (\tau - d) \leq 0 \mbox{ then } 0 \mbox{ else } s$.

In a client operating system, the instantaneous management of penalty function modifications should be highly automated by the system to avoid unduly burdening the user. Parameters could be set similarly to how priorities are set in some systems today. Applications are grouped into \emph{interaction classes} based on application type and the interaction class defines the deadline and slope.  In our implementation we set the parameters manually. However, future work is to experiment with alternative methods for \emph{learning} the deadline and slope.


As an application grows or diminishes in importance, its penalty function can be modified accordingly.  Penalty function adjustment is most likely to occur in transitions between operating scenarios.  For example, when unplugging a device all of the background activities could have their slopes significantly reduced to save battery life.

\subsection*{Power and Battery Energy}
Recall that we manage power and battery energy with an artificial application named Application 0 which receives all resources not allocated to other applications. Application 0's response time function is similar to the other applications' response time functions.  The function inputs are resource allocations just as with the other applications.  However, the function output is system power rather than response time.   To create the RTF, system power can be measured directly from on-chip energy counters in systems where they are available or from a power meter.  These models can be built in advance, during a training phase or online while the system runs, just as with the application RTFs.  Alternatively, the model could be part of the operating system platform-specific information.

Although system power may not be perfectly convex in reality, approximating it to be convex is reasonable because the convex model still captures the general behavior that idling a resource should not increase power.  As a result, Application 0 still fills its purpose of keeping applications from using additional resources that have poor performance/power ratios.

The penalty function can be adjusted to represent how important saving power and energy is in the current operating scenario.  For example, if a mobile device is plugged in to a charger than perhaps the penalty slope should be set very low.  However, if the device is unplugged the penalty function could be changed to have a power limit (a ``deadline'') and a steep slope that together establish a power cap for the system.  In this way the system can control the remaining hours the battery will last.

