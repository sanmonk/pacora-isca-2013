\section{Dynamic Optimization}

%IV.	Dynamic Optimization%	a.	Gradient Descent w/ Backtracking Search
%		i. boundary conditions clean up%	b.	Dealing with Fractional Results

\subsection*{On-line Response Time Modeling}

For the moment, assume the model norm $p = 1$ and suppose several response time measurements have already been made using a variety of resource allocations to begin optimizing the application response time.  After enough measurements, discovery of the model parameters $w$ that define the function $\tau$ can be based on a solution to the over-determined linear system
$t=Dw$
where $t$ is a column vector of actual response times measured for the process
and $D$ is a matrix whose ith row $D_{i,*}$ contains the reciprocals of the amplified bandwidth allocations
that generated the corresponding response time measurement $t_i$.
Estimating $w$ is relatively straightforward: a least-squares solution accomplished via
\emph{Q-R factorization}\cite{GoVL} of $D$ will determine the $w$ that minimizes the \emph residual error $\|t - Dw\|_2 = \|\varepsilon\|_2$.
The solution proceeds as follows:
\begin{eqnarray*}
t    &=& Dw  + \varepsilon    \\
        &=& QRw + \varepsilon    \\
Q^Tt &=& Rw  + Q^T\varepsilon
\end{eqnarray*}

It is not always necessary to materialize the orthogonal matrix $Q^T = Q^{-1}$;
the individual elementary orthogonal transformations (Householder reflections or Givens rotations)
that triangularize $R$ by progressively zeroing out partial columns of $D$ can simultaneously be applied to $t$.
The elements of the resulting vector $Q^Tt$ that correspond to zero rows in $R$ comprise $Q^T\varepsilon$.
Since $Rw$ exactly equals the upper part of $Q^Tt$, the upper part of $Q^T\varepsilon$ is zero. The residual error for the $t_i$
can be found by premultiplying $Q^T\varepsilon$ by $Q$.

Suppose a different model norm $p$ is desired.  If $p = 2$, we might first square each measurement in $t$
and each reciprocal bandwidth term in $D$ and then follow the foregoing procedure.
The elements of the result $w$ will be squares as well, and the 2-norm of the difference in the squared quantities will be minimized.  This is not the same as minimizing the 4-norm; what is being minimized is $\|t^2 - D^2w^2\|_2$.

\subsection*{Incremental Least Squares}

As resource allocation continues, more measurements will become available to augment $t$ and $D$.
Moreover, older data may become a poor representation of the current behavior of the process if its characteristics have changed, presumably as reflected in $Q^T\varepsilon$. .
What is needed is a factorization $\tilde{Q}\tilde{R}$ of a new matrix $\tilde{D}$
derived from $D$ by dropping a row, perhaps from the bottom,
and adding a row, perhaps at the top.
Corresponding elements of $t$ are dropped and added to form $\tilde{t}$.

The matrices $\tilde{Q}$ and $\tilde{R}$ can be generated by applying Givens rotations
in the way described in Section 12 of \cite{GoVL} to \emph{downdate} or \emph{update} the factorization
much more cheaply than recomputing it \emph{ab initio}.
The method requires retention and maintenance of $Q^T$ but not of $D$.
Every update in PACORA is preceded by a downdate that makes room for it.
Downdated rows are \emph{not} always the oldest (bottom) ones, but
an update always adds a new top row.
For several reasons, the number of rows $m$ in $R$
will be maintained at twice the number of columns $n$.
Rows selected for downdating will always be in the lower $m - n$ rows of $R$,
guaranteeing that the most recent $n$ updates are always part of the model.

Downdating makes an instructive example. A downdate applies
a sequence of Givens rotations to the rows of $Q^T$.
The rotations are calculated to set every $Q^T_{i,dd}$, $i \neq dd$ to zero.
In the end only the diagonal element $Q^T_{dd,dd}$ of column $dd$ will be nonzero.
Since $Q^T$ is still orthogonal, the non-diagonal elements of row $dd$ must also be zero
and the diagonal element will have absolute value 1.
These same rotations are concurrently applied to the elements of $Q^T t$ and to the rows of $R$ $(= Q^T D)$
to reflect the effect that these transformations had on $Q^T$.

It is crucial to select the row pairs and the order of rotations that will
preserve the upper triangular structure of $R$ while zeroing almost all of a column of $Q^T$.
Since $dd$ is below the diagonal of $R$ it initially will contain only zeros.
It therefore suffices to rotate every non-$dd$ row with row $dd$, proceeding from bottom to top.
The first $m - n - 1$ rotations will keep row $R_{dd,*}$ entirely zero,
and the remaining $n$ rotations will introduce nonzeros in $R_{dd,*}$ from right to left.
The effect on $R$ will be to replace zero elements by nonzero elements only in row $dd$.
At this point, except for a possible difference in overall sign, $R_{dd,*} = D_{dd,*}$.

Now the rows from the top down through $dd$ of the modified matrices $Q^Tt$ and $R$ and both the rows and columns of the new $Q^T$
are circularly shifted one position down, moving row $dd$ to the top (and column $dd$ of $Q^T$ to the left edge).
The following picture is the result:
\begin{displaymath}
\begin{array}{lll}
    \left[\begin{array}{cc}
      \pm1  &  0 \\
      0     &  \tilde{Q}^T
   \end{array}\right]
   \left[\begin{array}{c}
      t_{dd} \\
      \tilde{t}
   \end{array}\right]
   &=&
   \left[\begin{array}{c}
      \pm D_{dd,*} \\
      \tilde{R}
   \end{array}\right] w
   \\
   \\
   &+&
   \left[\begin{array}{cc}
      \pm1  &  0 \\
      0     &  \tilde{Q}^T
   \end{array}\right]
   \left[\begin{array}{c}
      \varepsilon_{dd} \\
      \tilde{\varepsilon}
   \end{array}\right]
\end{array}
\end{displaymath}
The top row has thus been decoupled from the rest of the factorization and may either be deleted or updated with new data.

The update process more or less reverses these steps, adding a new top row to R and $t$ and a row and column to $Q^T$.
Then $R$ is made upper triangular once more by a sequence of Givens rotations that zero its sub-diagonal elements
(formerly the diagonal elements of $\tilde{R}$).
These rotations are applied not just to $R$ $(= Q^T D)$ but also to $Q^Tt$ and of course to $Q^T$ itself.

\subsection*{Non-negativity}

The solution $w$ to $t \approx QRw$ must have no negative components to guarantee convexity of the response time model.
If a resource allocation is associated with multiple $w_j$
or if measured response time increases with the allocation,
then negative $w_j$ may occur.

The need for non-negative solutions to least-squares linear algebra problems is common,
so much so that it has a name: \emph{Non-Negative Least Squares}, or NNLS.
There are several well-known techniques \cite{ChPl},
but since the method proposed here for online model maintenance calls for
incremental downdates and updates to rows of $Q^T$, $Q^Tt$ and $R$,
the NNLS problem is handled with a complementary scheme that
downdates and updates the \emph{columns} of $R$ incrementally,
somewhat in the style of Algorithm 3 in \cite{LuDu}.
The scheme is too complex to be adequately described here.

\subsection*{Model Rank Preservation}

If care is not taken in the allocation process,
the rows of $R$ may become linearly dependent
to such an extent that its rank is insufficient to determine $w$.
This might be the result of repetitions in resource assignment updates,
perhaps caused by small process response time fluctuations.
There are several possible ways to avoid this \emph{rank-deficiency} problem.
The characteristics of $R$ depend on both the resource optimization trajectory and the
choices made in the downdate-update algorithm.
In particular, deciding whether to downdate the bottom row of $R$ or some ``younger'' row
will depend on whether the result would become rank-deficient.
This approach decouples allocation optimization from performance model maintenance
and places responsibility upon the latter to always keep enough history to determine $w$.

Deciding in advance whether downdating a row of $R$ will reduce its rank
is equivalent to predicting whether one of the Givens rotations, when applied to $R$,
will zero or nearly zero a diagonal entry of $R$.
This is particularly easy to discover because $dd$, the row to be downdated, is initially all zeros in $R$,
\emph{i.e.} in the lower part of the matrix.
In this situation a diagonal entry of $R$, $R_{i,i}$ say, will be compromised if and only if the
cosine of the Givens rotation that involves rows $dd$ and $i$ is nearly zero.
The result will be an interchange of the zero in $R_{dd,i}$ with the nonzero diagonal element $R_{i,i}$.
$R_{dd,i}$ is zero before the rotation because
$R$ was originally upper triangular and prior rotations only involved row subscripts greater than $i$.

PACORA keeps track of the sequence of values in $Q^T_{dd,dd}$ without actually changing $Q^T$
so that if the downdate at location $dd$ is eventually aborted there is nothing to undo.
It is also possible to remember the sines and cosines of the sequence of rotations
so they don't have to be recomputed if success ensues.
A rank-preserving row to downdate will always be available as long as $R$ is sufficiently ``tall''.
Having at least twice as many rows as columns is enough since the number of available rows to downdate
matches or exceeds the maximum possible rank of $R$.

\subsection*{Outliers and Phase Changes}

Some response time measurements may be ``noisy'' or even erroneous.
A weakness of least-squares modeling is the high importance it gives to outlying values.
On the other hand, when an application changes phase it is important to adapt quickly,
and what looks like an outlier when it first appears may be a harbinger of change.
What is needed is a way to discard either old or outlying data
with a judicious balance between age and anomaly.

The downdating algorithm accomplishes this by weighting the errors in $\varepsilon = Q(Q^Tt - Rw)$
between the predicted response times $\tau$ and the measured ones $t$ by a factor
that increases exponentially with the age $g(i)$ of the $i$th error $\varepsilon_i$.
Age can be modeled coarsely by the number of time quanta of some size since the measurement;
PACORA simply lets $g(i) = i$.
The weighting factor for the $i$th row is then $\eta^{g(i)}$ where $\eta$ is a constant somewhat greater than 1.
The candidate row to downdate is the row with the largest weighted error, \emph{i.e.}
\begin{displaymath}
dd = \arg\max_i \varepsilon_i \cdot \eta^{g(i)}
\end{displaymath}

\subsection*{Penalty Optimization}

Convex optimization is simplest when it is unconstrained.
Extending the response time model functions to all of $\Re^n$
moves the requirement that allocations must be positive into the objective function,
and introducing Process 0 for slack resources turns the affine inequalities into equalities:

\begin{eqnarray*}
& \makebox[1in][r]{Minimize}   & \sum_{p\epsilon P} {\pi_p(\tau_p(a_{p,1}\ldots a_{p,n}))}  \\
& \makebox[1in][r]{Subject to} & \sum_{p\epsilon P} a_{p,r} = A_r, r = 1,\ldots n           \\
\end{eqnarray*}

The only remaining constraints are those on the $a_{p,r}$.
These can be removed by letting the $a_{p,r}$ be unbounded above for $p \neq 0$
and changing the domain of $\tau_0$  to be the whole resource allocation matrix.
The definition of $\tau_0$ might take the form

\begin{eqnarray*}
\tau_0 &=& \sum_r d_r a_{0,r}     \\
       &=& \sum_r d_r ( A_r - \sum_{p \neq 0} a_{p,r} )
\end{eqnarray*}

where $d_r$ is the (constant) power dissipation associated with resource $r$.
However,if any of the allocations $a_{0,r}$ is negative then $\tau_0$  should instead return the value $+\infty$.
This modification of the objective function transforms the resource allocation problem
to unconstrained convex optimization.

The penalty optimization algorithm used in PACORA is descent via backtracking line search along the negative gradient direction \cite{BoVa}.
This algorithm rejects and refines any step that yields insufficient relative improvement in the objective function,
so infinite values from infeasible allocations will automatically be avoided by the search.
The negative gradient $-\nabla\pi$ of the overall objective function $\pi$
with respect to the resource allocations $a$
is computed analytically from the response time models and penalty functions.
When a component of this overall gradient is negative,
it means the penalty will be reduced by increasing the associated allocation if possible.
The gradient search at the boundaries of the feasible region
must ignore components that lead in infeasible directions;
these can be detected by noting whether for some $p$ and $r$, $a_{p,r} = 0$ with $(-\nabla\pi)_{p,r} > 0$.
In such cases, the associated step component should be set to zero.
Since the only constraint is variable positivity
then either the variable or its gradient component will be zero at a solution point; see \cite{BoVa}, page 142.

The rate of convergence of gradient descent depends on how well the sublevel sets of the objective function
are conditioned (basically, how “spherical” they are).
Conditioning will improve if resource allocation units are scaled to make their relative effects on $t$ similar.
For example, when compared with processor allocation units,
memory allocation units of 4MB are probably a better choice than 4 KB.
In addition, penalty function slopes should not differ by more than perhaps two orders of magnitude.
If these measures prove insufficient, stronger preconditioners can be used.
