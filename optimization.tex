\section{Dynamic Optimization}

%IV.	Dynamic Optimization
%	a.	Gradient Descent w/ Backtracking Search
%		i. boundary conditions clean up
%	b.	Dealing with Fractional Results

\subsection*{On-line Response Time Modeling}

For the moment, assume the model norm $p = 1$ and suppose several response time measurements have already been made using a variety of resource allocations to begin optimizing the application response time.  After enough measurements, discovery of the model parameters $w$ that define the function $\tau$ can be based on a solution to the over-determined linear system
$t=Dw$
where $t$ is a column vector of actual response times measured for the process
and $D$ is a matrix whose ith row $D_{i,*}$ contains the reciprocals of the amplified bandwidth allocations
that generated the corresponding response time measurement $t_i$.
Estimating $w$ is relatively straightforward: a least-squares solution accomplished via
\emph{Q-R factorization}\cite{GoVL} of $D$ will determine the $w$ that minimizes (half the)
square of the \emph residual error $1/2 \|Dw - t\|^2_2 = 1/2 \|\varepsilon\|^2_2$.
The solution proceeds as follows:
\begin{eqnarray*}
t     &=& Dw  - \varepsilon    \\
      &=& QRw - \varepsilon    \\
Q^Tt  &=& Rw  - Q^T\varepsilon
\end{eqnarray*}

It is not always necessary to materialize the orthogonal matrix $Q^T = Q^{-1}$;
the individual elementary orthogonal transformations (Householder reflections or Givens rotations)
that triangularize $R$ by progressively zeroing out partial columns of $D$ can simultaneously be applied to $t$.
The elements of the resulting vector $Q^Tt$ that correspond to zero rows in $R$ comprise $-Q^T\varepsilon$.
Since $Rw$ exactly equals the upper part of $Q^Tt$, the upper part of $Q^T\varepsilon$ is zero. The residual error for the $t_i$
can be found by premultiplying $Q^T\varepsilon$ by $Q$.

Suppose a different model norm $p$ is desired.  If $p = 2$, we might first square each measurement in $t$
and each reciprocal bandwidth term in $D$ and then follow the foregoing procedure.
The elements of the result $w$ will be squares as well, and the 2-norm of the difference in the squared quantities will be minimized.  This is not the same as minimizing the 4-norm; what is being minimized is $1/2\|\mbox{diag}(Dww^TD^T - tt^T)\|^2_2$.

\subsection*{Incremental Least Squares}

As resource allocation continues, more measurements will become available to augment $t$ and $D$.
Moreover, older data may become a poor representation of the current behavior of the process if its characteristics have changed,
presumably as reflected in $Q^T\varepsilon$.
What is needed is a factorization $\tilde{Q}\tilde{R}$ of a new matrix $\tilde{D}$
derived from $D$ by dropping a row, perhaps from the bottom,
and adding a row, perhaps at the top.
Corresponding elements of $t$ are dropped and added to form $\tilde{t}$.

The matrices $\tilde{Q}$ and $\tilde{R}$ can be generated by applying Givens rotations
in the way described in Section 12 of \cite{GoVL} to \emph{downdate} or \emph{update} the factorization
much more cheaply than recomputing it \emph{ab initio}.
The method requires retention and maintenance of $Q^T$ but not of $D$.
Every update in PACORA is preceded by a downdate that makes room for it.
Downdated rows are \emph{not} always the oldest (bottom) ones, but
an update always adds a new top row.
For several reasons, the number of rows $m$ in $R$
will be maintained at twice the number of columns $n$.
Rows selected for downdating will always be in the lower $m - n$ rows of $R$,
guaranteeing that the most recent $n$ updates are always part of the model.

Downdating makes an instructive example. A row downdate operation applies
a sequence of Givens rotations to the rows of $Q^T$.
The rotations are calculated to set every $Q^T_{i,dd}$, $i \neq dd$ to zero.
In the end only the diagonal element $Q^T_{dd,dd}$ of column $dd$ will be nonzero.
Since $Q^T$ remains orthogonal, the non-diagonal elements of row $dd$ will also have been zeroed automatically
and the diagonal element will have absolute value 1.
These same rotations are concurrently applied to the elements of $Q^T t$ and to the rows of $R$ $(= Q^T D)$
to reflect the effect that these transformations had on $Q^T$.

It is crucial to select pairs of rows and an order of rotations that preserves the upper triangular structure of $R$
while zeroing all but the diagonal entry of the chosen column $dd$ of $Q^T$.
Since $dd$ is always below the diagonal of $R$ it initially will contain only zeros.
It is therefore sufficient to rotate every non-$dd$ row with row $dd$, proceeding from bottom to top.
The first $m - n - 1$ rotations will keep row $R_{dd,*}$ entirely zero,
and the remaining $n$ rotations will introduce nonzeros in $R_{dd,*}$ from right to left.
The effect on $R$ will be to replace zero elements by nonzero elements only within row $dd$.
At this point, except for a possible difference in overall sign, $R_{dd,*} = D_{dd,*}$.

Now the rows from 0 down through $dd$ of the modified matrices $Q^Tt$ and $R$ and both the rows and columns of the modified $Q^T$
are circularly shifted by one position, moving row $dd$ to the top (and column $dd$ of $Q^T$ to the left edge).
The following is the result:
\begin{displaymath}
\begin{array}{lll}
    \left[\begin{array}{cc}
      \pm1  &  0 \\
      0     &  \tilde{Q}^T
   \end{array}\right]
   \left[\begin{array}{c}
      t_{dd} \\
      \tilde{t}
   \end{array}\right]
   &=&
   \left[\begin{array}{c}
      \pm D_{dd,*} \\
      \tilde{R}
   \end{array}\right] w
   \\
   \\
   &-&
   \left[\begin{array}{cc}
      \pm1  &  0 \\
      0     &  \tilde{Q}^T
   \end{array}\right]
   \left[\begin{array}{c}
      \varepsilon_{dd} \\
      \tilde{\varepsilon}
   \end{array}\right]
\end{array}
\end{displaymath}
The top row has thus been decoupled from the rest of the factorization and may either be deleted or updated with new data.

The update process more or less reverses these steps, adding a new top row to $R$ and $t$ and a row and column to $Q^T$.
Then $R$ is made upper triangular once more by a sequence of Givens rotations that zero its sub-diagonal elements
(formerly the diagonal elements of $\tilde{R}$).
These rotations are applied not just to $R$ but also to $Q^Tt$ and of course to $Q^T$ itself.

\subsection*{Non-Negativity}

To guarantee convexity of the response time model, the solution $w$ to $t \approx QRw$ must have no negative components.
Intuitively, when a resource is associated with more than a single $w_j$
or measured response time increases with allocation then negative $w_j$ may occur.

A requirement for non-negative solutions to least-squares linear algebra problems is common,
so much so that it has a name: \emph{Non Negative Least Squares}, or NNLS.
There are several well-known techniques\cite{ChPl},
but since the method proposed here for online model maintenance calls for
incremental downdates and updates to rows of $Q^T$, $Q^Tt$ and $R$,
the NNLS problem is handled with a complementary scheme
based on the \emph{active-set} method\cite{LaHa} that
also downdates and updates the \emph{columns} of $R$ incrementally,
roughly in the spirit of Algorithm~3 in~\cite{LuDu}.
However, PACORA's algorithm cannot discard downdated columns of $R$
because subsequent \emph{row} updates and downdates must have due effect
on these columns to allow their reintroduction via column updates as necessary.

The active-set NNLS method is based on the idea that since the only constraints are variable positivity
then for all components either the variable or its gradient will be zero at a solution point; see~\cite{BoVa}, page~142.
The active set, denoted by $z$, comprises the column subscripts $j$ for which the variable $w_j$ is zero and the gradient $v_j$ is positive.
If a column $j$ not currently in $z$ happens to acquire a negative $w_j$ after a back-solve, $w_j$ is zeroed,
$j$ is moved into $z$ and column $j$ is downdated in $R$ (thereby making the gradient become positive there).
Conversely, if a column in $z$ happens to acquire a negative gradient $v_j$ it is removed from $z$ and updated in $R$,
allowing it to become positive as it reduces the objective function.
The set $z$ is implemented as a bit vector.

After initial acquisition of data and $QR$ factorization, each step of PACORA's NNLS algorithm
combines incremental row and column downdates and updates as follows:

\begin{pseudocode}{IncrementalNNLS}{t_0,d_0}
\LOCAL{w,v,Q^T,Q^Tt,R,z,prevz}                              \\
Q^T,Q^Tt,R \GETS \textsc{DowndateRow}(Q^T,Q^Tt,R,z)         \\
Q^T,Q^Tt,R \GETS \textsc{UpdateRow}(t_0,d_0,Q^T,Q^Tt,R,z)   \\
w \GETS \textsc{BackSolve}(Q^Tt,R,z)                        \\
v \GETS \textsc{ComputeGradient}(Q^Tt,R,z)                  \\
\REPEAT     
  prevz \GETS z                                             \\
  \IF w_j < 0 \AND \not z_j \textbf{ for some }j \THEN
  \BEGIN
    Q^T,Q^Tt,R \GETS \textsc{DowndateCol}(j,Q^T,Q^Tt,R,z)   \\
    z_j \GETS \TRUE                                         \\
    w \GETS \textsc{BackSolve}(Q^Tt,R,z)                    \\
    v \GETS \textsc{ComputeGradient}(Q^Tt,R,z)              \\
  \END
  \ELSEIF v_j < 0 \AND z_j \textbf{ for some }j \THEN
  \BEGIN
    Q^T,Q^Tt,R \GETS \textsc{UpdateCol}(j,Q^T,Q^Tt,R,z)     \\
    z_j \GETS \FALSE                                        \\
    w \GETS \textsc{BackSolve}(Q^Tt,R,z)                    \\
    v \GETS \textsc{ComputeGradient}(Q^Tt,R,z)              \\
  \END                                                      \\
\UNTIL z = prevz                                            \\ 
\RETURN{w,v}                                                \\     
\end{pseudocode}
 
When a column $j$ is downdated in $R$ because $w_j < 0$, $j$ is added to $z$ and $w_j$ is set to zero.
The $j$th element of $Q^Tt$ now becomes a nonzero part of the error $-Q^T\varepsilon$.
To reflect this last requirement, the $j$th row of $R$ must be cleared except for $R_{j,j}$ by performing
a sequence of Givens rotations between $R_{j,*}$ and the rows $R_{k,*}$ for $k$ increasing from $j+1$ up to $n-1$.
As usual, these rotations are also applied to $Q^Tt$ (adjusting the error in its $j$th element) and to $Q^T$ itself.
In effect, these rotations remove $j$ from the upper triangle of $R$.
The result is a ``spike'' of nonzeros in the column $R_{*,j}$ extending below the diagonal to row $n-1$.
The loop that back-solves for $w$ contains a test that simply sets $w_j$ to zero if $z_j$ is true (\emph{i.e.} set).
This measure leaves $w_j$ zero rather than setting it to $(Q^Tt)_j/R_{j,j}$ as would otherwise happen.

Removals from $z$ are based on the gradient $v$ of the objective function, namely
\begin{eqnarray*}
v &=& 1/2\nabla\|Dw - t\|^2_2 \\
  &=& D^T(Dw - t)             \\
  &=& R^TQ^T(QRw - t)         \\
  &=& R^T(Rw - Q^Tt)          \\
  &=& -R^TQ^T\varepsilon.
\end{eqnarray*}
If for some $j$ in $z$ the inner product $v_j$ of the spike $R^T_{j,*}$ and $-Q^T\varepsilon$ is negative,
$w_j$ must be permitted to increase and so column $j$ is removed from $z$ and updated in $R$.
This update reverses the downdating steps by eliminating the spike below $R_{j,j}$
via a sequence of Givens rotations between row $R_{j,*}$ and rows $R_{k,*}$ with $k$ decreasing down to $j+1$.
As usual, the rotations are also applied to $Q^Tt$ and $Q^T$.
A new gradient computation and new back-solve for $w$ is clearly necessary after either downdates or updates to columns of $R$.

Downdates of \emph{rows} of $R$ in the presence of spikes in its columns present no special challenge
as long as the bottom-to-top order of Givens rotations is maintained.
Row updating in the presence of nonempty $z$ and column spikes is somewhat more challenging.
The downdated columns are associated with row subscripts as well because of the essentially triangular nature of $R$.
To keep these elements in place, the rotations that remove the sub-diagonal nonzeros in row-updating $R$
are made to be full exchanges (a rotation with cosine = -1) when the affected column is in $z$.
This keeps the row subscripts of elements of $z$ the same as before the row update and does not zero the sub-diagonal at the spike.
The spikes now extend one row lower than formerly even after all of the subdiagonals are eliminated,
and additional row updates will extend them still further.  The result is that column updates may have a longer spike to eliminate,
possibly extending upward from the bottom row of $R$.

\subsection*{Model Rank Preservation}

If care is not taken in the allocation process,
the rows of $R$ may become linearly dependent
to such an extent that its rank is insufficient to determine $w$.
This might be the result of repetitions in resource assignment updates.
There are several possible ways to avoid this \emph{rank-deficiency} problem.
The characteristics of $R$ depend on both the resource optimization trajectory and the
choices made in the downdate-update algorithm.
In particular, deciding whether to downdate the bottom row of $R$ or some ``younger'' row
will depend on whether the result would become rank-deficient.
This approach decouples allocation optimization from performance model maintenance
and places responsibility upon the latter to always keep enough history to determine a $w$.

Deciding in advance whether downdating a row of $R$ will reduce its rank
is equivalent to predicting whether one of the Givens rotations, when applied to $R$,
will zero or nearly zero a diagonal entry of $R$.
This is particularly easy to discover because $dd$, the row to be downdated, is initially all zeros in $R$,
\emph{i.e.} in the lower part of the matrix.
In this situation a diagonal entry of $R$, $R_{i,i}$ say, will be compromised if and only if the
cosine of the Givens rotation that involves rows $dd$ and $i$ is nearly zero.
The result will be an interchange of the zero in $R_{dd,i}$ with the nonzero diagonal element $R_{i,i}$.
$R_{dd,i}$ is zero before the rotation because
$R$ was originally upper triangular and prior rotations only involved row subscripts greater than $i$.

PACORA keeps track of the sequence of values in $Q^T_{dd,dd}$ without actually changing $Q^T$
so that if the downdate at location $dd$ is eventually aborted there is nothing to undo.
It is also possible to remember the sines and cosines of the sequence of rotations
so they don't have to be recomputed if success ensues.
A rank-preserving row to downdate will always be available as long as $R$ is sufficiently ``tall''.
Having at least twice as many rows as columns is enough since the number of available rows to downdate
matches or exceeds the maximum possible rank of $R$.

\subsection*{Outliers and Phase Changes}

Some response time measurements may be ``noisy'' or even erroneous.
A weakness of least-squares modeling is the high importance it gives to outlying values.
On the other hand, when an application changes phase it is important to adapt quickly,
and what looks like an outlier when it first appears may be a harbinger of change.
What is needed is a way to discard either old or outlying data
with a judicious balance between age and anomaly.

The downdating algorithm accomplishes this by weighting the errors in $\varepsilon = Q(Q^Tt - Rw)$
between the predicted response times $\tau$ and the measured ones $t$ by a factor
that increases exponentially with the age $g(i)$ of the $i$th error $\varepsilon_i$.
Age can be modeled coarsely by the number of time quanta of some size since the measurement;
PACORA simply lets $g(i) = i$.
The weighting factor for the $i$th row is then $\eta^{g(i)}$ where $\eta$ is a constant somewhat greater than 1.
The candidate row to downdate is the row with the largest weighted error, \emph{i.e.}
\begin{displaymath}
dd = \arg\max_i |\varepsilon_i| \cdot \eta^{g(i)}
\end{displaymath}

\subsection*{Penalty Optimization}

Convex optimization is simplest when it is unconstrained.
Extending the response time model functions to all of $\Re^n$
moves the requirement that allocations must be positive into the objective function,
and introducing Process 0 for slack resources turns the affine inequalities into equalities:
\begin{eqnarray*}
& \makebox[1in][r]{Minimize}   & \sum_{p\epsilon P} {\pi_p(\tau_p(a_{p,1}\ldots a_{p,n}))}  \\
& \makebox[1in][r]{Subject to} & \sum_{p\epsilon P} a_{p,r} = A_r, r = 1,\ldots n           \\
\end{eqnarray*}

The only remaining constraints are those on the $a_{p,r}$.
These can be removed by letting the $a_{p,r}$ be unbounded above for $p \neq 0$
and changing the domain of $\tau_0$  to be the whole resource allocation matrix.
The definition of $\tau_0$ might take the form
\begin{eqnarray*}
\tau_0 &=& \sum_r \Delta_r a_{0,r}     \\
       &=& \sum_r \Delta_r ( A_r - \sum_{p \neq 0} a_{p,r} )
\end{eqnarray*}
where $\Delta_r$ is the (constant) power dissipation associated with resource $r$.
However,if any of the allocations $a_{0,r}$ is negative then $\tau_0$  should instead return the value $+\infty$.
This modification of the objective function transforms the resource allocation problem
to unconstrained convex optimization.

The penalty optimization algorithm used in PACORA is descent via backtracking line search along the negative gradient direction \cite{BoVa}.
This algorithm rejects and refines any step that yields insufficient relative improvement in the objective function,
so infinite values from infeasible allocations will automatically be avoided by the search.
The negative gradient $-\nabla\pi$ of the overall objective function $\pi$
with respect to the resource allocations $a$
is computed analytically from the response time models and penalty functions.
When a component of this overall gradient is negative,
it means the penalty will be reduced by increasing the associated allocation if possible.
The gradient search at the boundaries of the feasible region
must ignore components that lead in infeasible directions;
these can be detected by noting whether for some $p$ and $r$, $a_{p,r} = 0$ with $(-\nabla\pi)_{p,r} > 0$.
In such cases, the associated step component is set to zero.

The rate of convergence of gradient descent depends on how well the sublevel sets of the objective function
are conditioned (basically, how “spherical” they are).
Conditioning will improve if resource allocation units are scaled to make their relative effects on $t$ similar.
For example, when compared with processor allocation units,
memory allocation units of 4MB are probably a better choice than 4 KB.
In addition, penalty function slopes should not differ by more than perhaps two orders of magnitude.
If these measures prove insufficient, stronger preconditioners can be used.
