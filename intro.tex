\section{Introduction}


The demands on modern systems have changed.  Users demand responsive applications often with high-quality multimedia that requires real-time guarantees.  Meeting this responsiveness goal is a challenge for all types of systems including cloud systems, databases, webservers, client operating systems, and emerging distributed embedded systems. Additionally battery life and system power are extremely important, thereby forcing systems to try to find more efficient ways to meet the quality-of-service demands of their workloads.

Ideally, applications, jobs, or queries with strict performance requirements should be given just enough system resources (\emph{e.g.,} nodes, processor cores, cache slices, memory pages, various kinds of bandwidth) to meet these requirements consistently, without unnecessarily siphoning resources from other applications. However, executing multiple parallel, real-time applications while satisfying  \emph{Quality-of-Service} (QoS) requirements is a complex optimization problem, particularly as modern hardware diversifies to include a variety of parallel architectures (\emph{e.g.,} multicore, gpus).  Historically operating systems have not provided useful mechanisms that implement stronger performance guarantees, so developers have been left with few alternatives to over-provisioning.  Resource allocation has been rather unsystematic making it difficult to reason about the expected response time of an application.

Consequently, predictability has traditionally been obtained at a significant expense by designing for the worst case and over-provisioning.  Evidence of this behavior can be found in current mobile and cloud systems.  In order to preserve responsiveness and battery life,  some mobile systems have gone so far as to limit which applications can run in the background~\cite{iOsDev}, despite the obvious concerns this raises for user experience.  Cloud computing providers routinely utilized their clusters at only 10\% to 50\% to keep the system responsive despite the additional operational costs of consuming electricity and the significant impact to the capital costs of the infrastructure~\cite{Barroso2009,Hennessy2011}. 

\fix{and forcing isolation, realtime}

Alternatively, by understanding application requirements through measurement and effectively adapting to these requirements, systems can provide predictable behavior without over-provisioning, allowing excess resources to be turned off or to be used opportunistically --gaining efficiency.  In this paper, we present \pacora, a resource allocation framework, which is designed to provide responsiveness guarantees to a simultaneous mix of high-throughput parallel, interactive, and real-time applications in an efficient, scalable manner. \pacora leverages convex optimization and application performance models to determine the optimal amount of each resource to give each application, enabling the system to make trade-offs between application QoS/responsiveness, system performance, and energy efficiency. 

The \pacora framework is applicable to many resource allocation scenarios including cloud providers determining how much to give each job to avoid violating SLAs, databases allocating resources to queries and distributed embedded systems allocation bandwidth between devices and sensors.  In this paper we choose to study client applications and implement the system in a general-purpose operating system because we believe this scenario has some of the most significant resource allocation challenges: the constantly changing environment requires low overhead and fast response times from \pacora;  shared resources create more interference between applications; and the applications are more likely to be written by domain experts, thus less highly optimized.

\fix{add traditional solutions don't work,  Pacora values: Application Specific, Resources Matter, Measurement, Optimization}

\fix{Add Performance Numbers}

%Responsiveness has been described by a single value (usually called a \emph{priority}) associated with a thread of computation and adjusted within the operating system by a variety of ad-hoc mechanisms.   Other shared resources either employ independent machinery (\emph{e.g.,} memory, caches), or are deemed so abundant as to require no explicit management at all (\emph{e.g.,} I/O, network bandwidth).
