\section{Introduction}


The demands on modern systems have changed in the Post-PC era.  Users demand application responsiveness. These applications often deliver high-quality multimedia that require real-time guarantees.  Meeting this responsiveness goal is a challenge for all types of systems including cloud systems, databases, webservers, client operating systems, and emerging distributed embedded systems. Additionally, battery life and system power are extremely important, thereby forcing systems to try to find more efficient ways to meet the needs of their workloads.

Applications, jobs, or queries with strict performance requirements should ideally be given just enough system resources (\emph{e.g.} nodes, processor cores, cache slices, memory pages, various kinds of bandwidth) to meet these requirements consistently, without unnecessarily siphoning resources from other applications. However, executing multiple parallel, real-time applications while satisfying  \emph{Quality-of-Service} (QoS) requirements is a complex optimization problem, particularly as modern hardware diversifies to include a variety of parallel architectures (\emph{e.g.,} multicore, gpus).  Operating systems have historically not provided useful mechanisms that implement stronger performance guarantees and resource allocation has been rather unsystematic, making it difficult to reason about the expected response time of an application.

Consequently, responsiveness has been obtained at significant expense by designing for the worst case and over-provisioning.  Evidence of this behavior can be found in current systems of all sizes.  OSs attempt to describe responsiveness with a single value (usually called a \emph{priority}) associated with a thread of computation and adjusted within the operating system by a variety of \emph{ad-hoc} mechanisms. Other shared resources either employ independent machinery (\emph{e.g.} memory, caches), or are deemed so abundant as to require no explicit management at all (\emph{e.g.} I/O, network bandwidth).
 Priority approaches have no mechanism to describe deadlines or the resources required to meet them and so must run the highest priority applications as fast as possible on all the resources requested.   As a result, interactive and real-time applications are often run needlessly fast with significant over-provisioning --- wasting power and energy and preventing other applications from using the resources.

Some mobile systems have gone so far as to limit which applications can run in the background~\cite{iOsDev} in order to preserve responsiveness and battery life, despite the obvious concerns this raises for user experience.  Cloud computing providers routinely utilize their clusters at only 10\% to 50\% to keep the system responsive despite the additional operational costs of consuming electricity and the significant impact to the capital costs of the infrastructure~\cite{Barroso2009,Hennessy2011}.   In some cases, clusters only run a single application on each cluster to avoid unexpected interference.  Similarly the realtime community has used completely separate systems for each application to provide QoS despite the high cost of specialization and low utilization with this approach.

In this paper, we present \pacora, a resource allocation framework designed to provide responsiveness guarantees to a simultaneous mix of high-throughput parallel, interactive, and real-time applications in an efficient, scalable manner.  Unlike traditional systems, \pacora considers all resource types when making decisions and continuously determines the complete set of resources each application will use.   It maintains application-specific performance models through measurement to help determine resource requirements and responds to dynamic changes in both the deadlines and the relative importance of meeting each of them. It leverages convex optimization to determine the quantity of each resource to give each application. This lets the system make trade-offs among application QoS/responsiveness, system performance, and energy efficiency.

We believe \pacora is applicable to many resource allocation scenarios including cloud providers determining how much to give each job to avoid violating SLAs, databases allocating resources to queries, and distributed embedded systems allocating bandwidth among devices and sensors.  In this paper we choose to study client systems and their applications, implementing \pacora in a general-purpose operating system, because we believe this scenario has some of the most significant resource allocation challenges: a constantly changing environment requiring low overhead and fast response times, shared resources that create more interference among the applications, and platforms that are too diverse to allow \emph{a priori} performance prediction.
% applications that are more likely to be written by domain experts, thus less highly optimized.

\fix{Add Performance Numbers}

In this paper we first present the architecture of \pacora in Section~\ref{sys_design}.  In Section~\ref{eval}, we evaluate \pacora's effectiveness at resource allocation. Section~\ref{dyn_opt} describes and evaluates our dynamic penalty optimization algorithm. Sections~\ref{app_func} and~\ref{model_creation} describe response time functions and their creation in detail.  Section~\ref{discuss} some challenges and possible solutions for \pacora. Section~\ref{related_work} discusses related work, and Section~\ref{conclusion} concludes.
