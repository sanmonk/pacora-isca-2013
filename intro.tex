\section{Introduction}


The demands on modern systems have changed.  Users demand responsive applications often with high-quality multimedia that requires real-time guarantees.  Meeting this responsiveness goal is a challenge for all types of systems including cloud systems, databases, webservers, client operating systems, and emerging distributed embedded systems. Additionally battery life and system power are extremely important, thereby forcing systems to try to find more efficient ways to meet the quality-of-service demands of their workloads.

Ideally, applications, jobs, or queries with strict performance requirements should be given just enough system resources (\emph{e.g.,} nodes, processor cores, cache slices, memory pages, various kinds of bandwidth) to meet these requirements consistently, without unnecessarily siphoning resources from other applications. However, executing multiple parallel, real-time applications while satisfying  \emph{Quality-of-Service} (QoS) requirements is a complex optimization problem, particularly as modern hardware diversifies to include a variety of parallel architectures (\emph{e.g.,} multicore, gpus).  Historically operating systems have not provided useful mechanisms that implement stronger performance guarantees and resource allocation has ben rather unsystematic, making it difficult to reason about the expected response time of an application. 

Consequently, predictability has traditionally been obtained at a significant expense by designing for the worst case and over-provisioning.  Evidence of this behavior can be found in current systems of all sizes.  OSs describe responsiveness with a single value (usually called a \emph{priority}) associated with a thread of computation and adjusted within the operating system by a variety of ad-hoc mechanisms. Other shared resources either employ independent machinery (\emph{e.g.,} memory, caches), or are deemed so abundant as to require no explicit management at all (\emph{e.g.,} I/O, network bandwidth).
 Priority approaches have no mechanism to understand deadlines or the resources required to meet a deadline and as such must run the highest priority applications as fast as possible on all the resources requested.   As a result, interactive and realtime applications are often run needlessly fast with significantly over-provisioned resources --wasting power and energy and preventing other applications from using the resources.  

Some mobile systems have gone so far as to limit which applications can run in the background~\cite{iOsDev} in order to preserve responsiveness and battery life,  , despite the obvious concerns this raises for user experience.  Cloud computing providers routinely utilized their clusters at only 10\% to 50\% to keep the system responsive despite the additional operational costs of consuming electricity and the significant impact to the capital costs of the infrastructure~\cite{Barroso2009,Hennessy2011}.   In some cases, clusters only run a single application on each cluster to avoid unexpected interference.  Similarly the realtime community has used completely separate systems for each application to provide QoS to applications despite the high-cost of specialization and low utilization with this approach.

\fix{Pacora values: Application Specific, Resources Matter, Measurement, Optimization, end to end qos}
Alternatively, by understanding application deadlines and resource requirements through measurement and effectively adapting to these requirements, systems can provide predictable behavior without over-provisioning, allowing excess resources to be turned off or to be used opportunistically --gaining efficiency.  In this paper, we present \pacora, a resource allocation framework, which is designed to provide responsiveness guarantees to a simultaneous mix of high-throughput parallel, interactive, and real-time applications in an efficient, scalable manner. \pacora leverages convex optimization and application performance models to determine the optimal amount of each resource to give each application, enabling the system to make trade-offs between application QoS/responsiveness, system performance, and energy efficiency. 

The \pacora framework is applicable to many resource allocation scenarios including cloud providers determining how much to give each job to avoid violating SLAs, databases allocating resources to queries and distributed embedded systems allocation bandwidth between devices and sensors.  In this paper we choose to study client applications and implement the system in a general-purpose operating system because we believe this scenario has some of the most significant resource allocation challenges: the constantly changing environment requires low overhead and fast response times from \pacora;  shared resources create more interference between applications; and the applications are more likely to be written by domain experts, thus less highly optimized.

\fix{Pacora values: Application Specific, Resources Matter, Measurement, Optimization}

\fix{Add Performance Numbers}

%Responsiveness has been described by a single value (usually called a \emph{priority}) associated with a thread of computation and adjusted within the operating system by a variety of ad-hoc mechanisms.   Other shared resources either employ independent machinery (\emph{e.g.,} memory, caches), or are deemed so abundant as to require no explicit management at all (\emph{e.g.,} I/O, network bandwidth).
