\chapter{PACORA Implementation in a Manycore Operating System}\label{tess_design_ch}
In this chapter, we present our implementation of \pacora in \tess OS, a manycore research operating system \cite{tess_resource, tess, tess_dac, tess_audio, tess_gui}.  We give an overview of \tess and why we chose it for our implementation.  We then discuss the details for building RTFs online in the operating system.  Finally, we present our implementation of the resource allocator using an ADMM optimization method.




To evaluate \pacora's ability to make real-time decisions in a real operating system, we implemented it in an in-house research operating system, \tess. We chose to implement in \tess rather than Linux for three reasons:
 \begin{enumerate}\itemsep0pt \parskip0pt \parsep5pt
\item \tess separates resource allocation from scheduling, so is
  closer to the OS architecture assumed by \pacora
\item \tess allows resource revocation, enabling \pacora to dynamically reallocate resources
\item \tess implements additional resource partitioning mechanisms,
  letting \pacora manage  more resource types
\end{enumerate}

 Further, modifying a full-fledged production OS such as Linux to investigate new resources management schemas is complex and
 requires more implementation effort than developing for our resource-centric OS.  For example, in our OS we wrote an Earliest
 Deadline First (EDF) scheduler that runs entirely in user-space in about 800 lines of code, contained in four files.  By contrast, support for
 EDF in Linux requires kernel modifications and substantially more code: the best-known EDF kernel patch for Linux, SCHED\_DEADLINE, has over 3500 modified lines in over 50 files.
We use the \tess implementation to test our implementations of the algorithms, measure the overhead and reaction times, and illustrate \pacora's ability to work in a real system. 

\section{\tess Overview}

This section briefly describes the key components of \tess OS~\cite{tess,tess_resource,tess_dac,tess_audio, tess_gui}.
The \tess kernel is a thin, hypervisor-like layer that provides support
for dynamic resource management.  It implements cell along with interfaces for user-level scheduling, resource
adaptation, and cell composition.  \tess currently runs on x86 hardware
platforms (\emph{e.g.,} with Intel's Sandy Bridge CPUs). 








\subsection{The Cell Model}\label{sec:cell-model}

In \tess, resources are distributed to QoS domains called
\emph{cells}, which are explicitly parallel, light-weight, performance-isolated containers
with guaranteed, user-level access to resources. The software running within each cell has full
user-level control of the cell's resources (\eg CPU cores, memory pages,
and I/O devices).  


\begin{figure}[tp]
%\vspace*{-0.2in}
\centering
%\rule{4cm}{3cm}
\includegraphics[width=1.0\columnwidth]{Figures/app-split-into-cells.pdf}
%\vspace*{-0.4in}
\caption{
Applications in \tess are created as sets of interacting components
hosted in different cells that communicate over channels.
Standard OS services (\eg the file service) are also hosted in cells and
accessed via channels.
}
\label{fig:app-split-into-cells}
%\vspace*{-0.15in}
\end{figure}

As depicted in Figure~\ref{fig:app-split-into-cells}, applications
in \tess are created by composing cells via \emph{channels}, which
provide fast, user-level asynchronous message-passing between cells.
Applications can then be split into performance-incompatible and
mutually distrusting cells with controlled communication. 


\tess OS implements cells on x86 platforms by partitioning resources
using \emph{space-time partitioning}~\cite{rushby99,lei03}, 
a multiplexing technique that divides the hardware into a sequence of
simultaneously-resident spatial partitions. 
Cores and other resources are
\textit{gang-scheduled}~\cite{gangsched1982,gangschedpatent}, so
cells provide to their hosted applications an environment that is very
similar to a dedicated machine.

\subsection{Resources and Services} \label{sec:soa} 
Partitionable resources include CPU cores, memory pages, and guaranteed
fractional services from other cells (\emph{e.g.,} a throughput reservation of 150~Mbps
from the network service).  
They may also include cache slices, portions of memory bandwidth, and fractions
of the energy budget, when hardware support is available
\cite{akesson07,lee08memqos,paolieri09,sanchez11}.

\tess also creates \emph{service cells} which have exclusive
control over devices, and encapsulate user-level device drivers. 
Each service can thus arbitrate access to its
enclosed devices to offer service guarantees to
other cells. \tess treats the services
offered by such \emph{service cells} as additional resources to be allocated to applications.

\tess currently has two such service cells implemented:
the \emph{Network Service}, which provides access to network adapters and
guarantees that the data flows are processed with the agreed levels of
throughput; and the \emph{GUI Service}, which provides a windowing system with
response-time guarantees for visual applications.  


\subsection{Two-Level Scheduling} 
\emph{Two-level scheduling}~\cite{leiner07,ober08} %,tess10} 
in \tess separates global decisions about allocation of resources \emph{to}
cells (\emph{first level}) from application-specific usage of resources
\emph{within} cells (\emph{second level}). 
Resource redistribution occurs at a coarse time scale to amortize the
decision-making cost and allow time for second-level scheduling decisions (made
by each cell) to become effective.

By separating resource management into dynamic allocation and QoS
enforcement, we are essentially adopting \emph{two-level
scheduling}~\cite{leiner07,ober08,tess_resource}.
While two-level scheduling has been investigated in the past,
leveraging this concept to address the issues that emerge in the dynamic
execution scenarios we outlined in this introduction raises complex challenges
that are often underestimated.  For this
reason, we evaluate a complete, detailed solution that incorporates
parallelism, QoS guarantees, and dynamic optimization with two-level
scheduling.  We directly address many of the issues that arise from this
challenge, such as timing for interacting control loops, application
resizing, and efficient gang scheduling.

Scheduling within cells functions purely at the user-level, as
close to the \emph{bare metal} as possible, improving efficiency and
eliminating unpredictable OS interference.  Our framework for preemptive
scheduling, called \emph{Pulse}, enables customization and support for a wide
variety of application-specific runtimes and schedulers without
kernel-level modifications. 
The user-level runtime within each cell can be tuned for a specific
application or application domain with a custom scheduling algorithm.
Using our user-level scheduler framework, \tess provides pre-canned
implementations for TBB~\cite{tbb07} and a number of scheduling algorithms,
including Global Round Robin (GRR), Earliest Deadline First
(EDF), and Speed Balancing~\cite{juggle2013}.  Others may be easily
constructed if necessary.  

 
In addition to support for timer interrupts, the Pulse API provides callbacks
for adaptation events to notify schedulers when the number of available cores
changes.  
These notifications permit resource-aware, application-specific management,
which is impossible with a centralized OS approach.
This capability eliminates the need
to build a \emph{one-size-fits-all} scheduler, thus sidestepping a difficult
design challenge~\cite{cfs-vs-bfs}.



\subsection{Adaptive Resource Allocation} \label{sec:rsc-alloc}

The decision of how to allocate
resources to applications is computed by a policy service, which uses models of
application behavior to predict how resources map to performance. The actual
distribution of resources to applications is carried out by the kernel, which
receives allocations from the policy service. Once applications are running,
performance counters and application heartbeats are used by the policy service
to adjust the resource allocations according to the application models.

\tess uses an adaptive resource allocation approach to assign resources
to cells.  This functionality is performed by the \emph{Resource Broker}, as
shown in Figure~\ref{fig:tess-arch}.  The Resource Broker distributes
resources to cells while attempting to satisfy competing application
performance targets and system-wide goals, such as deadlines met, energy
efficiency, and throughput.  It utilizes resource constraints,
application models, and current performance measurements as inputs to
this optimization.  Allocation decisions are communicated to the kernel
and services for enforcement.  The Resource Brokerabbrv reallocates resources,
for example, when a cell starts or finishes or when a cell significantly
changes performance.  It can periodically adjust allocations; the
reallocation frequency provides a tradeoff between adaptability (to
changes in state) and stability (of user-level scheduling).

The Resource Broker provides a resource allocation framework that supports
rapid development and testing of new allocation policies.  This
framework enables us to explore the potential of an ARCC-based OS for
providing QoS to individual applications while optimizing resource
distribution to achieve global objectives.  In
Section~\ref{sec:rab-service}, we discuss two policies we have
implemented to demonstrate this potential.


\begin{figure}[t]
\centering
%\rule{4cm}{3cm}
\includegraphics[width=0.885\linewidth]{Figures/NewPolicyFig-Tess}
%\vspace*{-0.1in}
\caption{
The \tess kernel implements \emph{cells} through \emph{spatial-partitioning}.
The \emph{Resource Broker} redistributes resources after consulting application-specific
\emph{heartbeats} and system-wide \emph{resource reports}.
}
\label{fig:tess-arch}
%\vspace*{-0.175in}
\end{figure}


  
  
   
 

\section{RTF Creation}\label{rtf_creation}
To create RTF models either at install time or online we use a convex
least-squares approach described below.  At install time, we use a
genetic algorithm, Audze-Eglasis Design of
Experiments\cite{bates-aes03}, to select the resource vectors to use
for training.  These vectors and their response times are fed into the
convex least-squares algorithm. Online model building uses data
from the application's response time history.

\subsection*{Data Collection and Model Creation Time}
There are many ways to collect the response time data for
applications. The user-level runtime scheduler is one possible source,
or the operating system could measure progress using performance
counters.  In our implementation, applications report their own
measured values; however, this solution was chosen simply as a way to
test the validity of the concept.  In a production operating system it
may not be a good idea because applications could lie about their
performance.  In a single-operator datacenter environment this might be less of a concern.

There are also many different possible moments to create response time functions.  RTFs could be created in advance and distributed with the application. This approach could make lots of sense for app stores since most of them cater to just a few platforms. RTFs could also be crowd-sourced and built in the cloud, which has the advantage making it easy to collect a diverse set of training points.  However, all of these approaches lack adaptability.  As a result, we have chosen to implement two solutions that collect data directly from the user's machine.  The first approach is to adapt to the system by collecting all of the training points at application install time and building the model then.  The most highly adaptive approach collects data continuously as the application runs, uses the data to modify the model training set, and rebuilds the model.  A hybrid approach may be the most effective: applications can begin with a generic model and improve it over time. The remainder of this Section describes our model creation process in detail.

In a client operating system, management of penalty function modifications
should be highly automated by the system to avoid unduly burdening the user.
As an application grows or diminishes in importance, its penalty function can be modified accordingly.
Adjustment is also likely to occur in transitions between operating scenarios.
For example, when unplugging a device all of the background activities could have their slopes significantly reduced to save battery life.

\subsection*{Least-Squares Minimization}
After enough measurements, discovery of the model parameters $w$ that define the function $\tau$
can be based on a solution to the over-determined linear system $t=Dw$,
where $t$ is a column vector of actual response times measured for the application
and $D$ is a matrix whose $i$th row $D_{i,*}$ contains the corresponding resource vector.
Estimating $w$ is relatively straightforward: a least-squares solution accomplished via
\emph{QR factorization}\cite{GoVL} of $D$ will determine the $w$ that minimizes the \emph residual error of
$\|Dw - t\|^2_2 =  \|Rw - Q^Tt\|^2_2$.
%The solution proceeds as follows:
%\begin{eqnarray*}
%t     &=& Dw  - \varepsilon    \\
%      &=& QRw - \varepsilon    \\
%Q^Tt  &=& Rw  - Q^T\varepsilon
%\end{eqnarray*}
%
The individual elementary orthogonal transformations, \emph{e.g.,} Givens rotations,
that triangularize $R$ by progressively zeroing out $D$'s sub-diagonal elements are simultaneously applied to $t$.
%The elements of the resulting vector $Q^Tt$ that correspond to zero rows in $R$ comprise $-Q^T\varepsilon$.
%Since $Rw$ exactly equals the upper part of $Q^Tt$, the upper part of $Q^T\varepsilon$ is zero. The residual error for the %$t_i$
%can be found by premultiplying $Q^T\varepsilon$ by $Q$.

%This formulation assumes a model norm $p = 1$. If a different model norm $p$ is desired, such as $p = 2$, we could first square %each measurement in $t$
%and each reciprocal bandwidth term in $D$ and then follow the foregoing procedure.
%The elements of the result $w$ will be squares as well, and the 2-norm of the difference in the squared quantities will be %minimized.  This is not the same as minimizing the 4-norm; what is being minimized is $1/2\|\mbox{diag}(Dww^TD^T - tt^T)\|^2_2$.

\subsection{On-line Response Time Modeling}
As resource allocation continues, more measurements will become available to augment $t$ and $D$.
Moreover, older data may poorly represent the current behavior of the application.
\pacora uses an incremental approach described below to replace old data and efficiently update RTFs.

\subsubsection*{Incremental Least-Squares}
What is needed is a factorization $\tilde{Q}\tilde{R}$ of a new matrix $\tilde{D}$
derived from $D$ by dropping a row, perhaps from the bottom,
and adding a row, perhaps at the top.
Corresponding elements of $t$ are dropped and added to form $\tilde{t}$.

The matrices $\tilde{Q}$ and $\tilde{R}$ can be generated by applying Givens rotations
as described in Section 12 of \cite{GoVL} to \emph{downdate} or \emph{update} the factorization
much more cheaply than recomputing it \emph{ab initio}.
The method requires retention and maintenance of $Q^T$ but not of $D$.
Every update in \pacora is preceded by a downdate that makes room for it.
Downdated rows are \emph{not} always the oldest (bottom) ones, but
an update always adds a new top row.
For several reasons, the number of rows $m$ in $R$
will be at least twice the number of columns $n$.
Rows selected for downdating will always be in the lower $m - n$ rows of $R$,
guaranteeing that the most recent $n$ updates are always part of the model.

%\subsubsection*{Non-Negativity}
%\begin{figure*}[!t]
%	\begin{center}	
%%		\includegraphics[width=.45\textwidth]{cluster_decision_points.pdf}
%		\caption{Performance of \pacora's NNLS algorithm}
%		\label{model_build_perf}
%	\end{center}
%\end{figure*}

To guarantee convexity of the RTF, the solution $w$ to $t \approx QRw$ must have no negative components.
Intuitively, when a resource is associated with more than a single $w_j$
or when the measured response time increases with allocation then negative $w_j$ may occur. \emph{Non-negative Least-Squares} problems (NNLS) are common linear algebra, and there are several well-known techniques\cite{ChPl}.
However since \pacora's online model maintenance calls for
incremental downdates and updates to rows of $Q^T$, $Q^Tt$ and $R$,
the NNLS problem is handled with a scheme
based on the \emph{active-set} method\cite{LaHa} that
also downdates and updates the \emph{columns} of $R$ incrementally,
roughly in the spirit of Algorithm~3 in~\cite{LuDu}.
However, \pacora's algorithm cannot ignore downdated columns of $R$
because subsequent \emph{row} updates and downdates must have due effect
on these columns to allow their later reintroduction via column updates as necessary.
This problem is solved by leaving the downdated columns in place,
skipping over them in maintaining and using the QR factorization.

The memory used in maintaining a model with $n$ weights is modest, $24n^2 + 21n + \textrm{O}(1)$ bytes.
For $n = 8$ this is under 2 KB, fitting nicely in L1 cache.
Our NNLS implementation takes \SI{4}{\micro\second} per update-downdate pair in \tess.
%Figure~\ref{model_build_perf} shows the performance of \pacora's NNLS algorithm.

\subsubsection*{Model Rank Preservation}
If care is not taken in downdating $R$, its rows may become so linearly dependent,
perhaps from repetitive resource allocations,
that determining a unique $w$ is impossible.
The rank of $R$ depends on both the resource optimization trajectory and the
choices made in the row downdate-update algorithm.
\pacora exploits the latter idea and simply avoids downdating any row that will make $R$ rank-deficient.

\subsubsection*{Outliers and Phase Changes}

%\begin{figure*}[!t]
%	\begin{center}	
%%		\includegraphics[width=.45\textwidth]{cluster_decision_points.pdf}
%		\caption{\pacora's online model creation algorithm adapting to a video application changing phases}
%		\label{phase_change}
%	\end{center}
%\end{figure*}

Some response time measurements may be ``noisy'' or even erroneous.
A weakness of least-squares modeling is the high importance it gives to outlying values.
On the other hand, when an application changes phase it is important to adapt quickly,
and what looks like an outlier when it first appears may be a harbinger of change.
What is needed is a way to discard either old or outlying data
with a judicious balance between age and anomaly.

The downdating algorithm accomplishes this by weighting the errors in $\varepsilon = Q(Q^Tt - Rw)$
between the predicted response times $\tau$ and the measured ones $t$ by a factor
that increases exponentially with the age $g(i)$ of the error $\varepsilon_i$.
Age can be modeled coarsely by the number of time quanta of some size since the measurement;
\pacora simply lets $g(i) = i$.
The weighting factor for the $i$th row is then $\eta^{g(i)}$ where $\eta$ is a constant somewhat greater than 1.
The candidate row to downdate is the row with the largest weighted error, \emph{i.e.,}
$dd = \arg\max_i |\varepsilon_i| \cdot \eta^{g(i)}$ and that does not reduce the rank of $R$.
%Figure~\ref{phase_change} shows \pacora's model creation algorithm adapting to a video application changing phases.


\subsubsection*{Power Response Modeling}
Recall that we manage power and battery energy with an artificial application named application 0 which receives all resources not allocated to other applications. Application 0's "response time" function is similar to the other applications' RTFs.  The function inputs are resource allocations just as with the other applications.  However, the function output is system power rather than response time.   To create the RTF, system power can be measured directly from on-chip energy counters in systems where they are available or from a power meter.  These models can be built in advance, during a training phase or online while the system runs, just as with the application RTFs.  Alternatively, the model could be part of the operating system's platform-specific information.

Although system power may not be perfectly convex in reality, forcing it to be convex is reasonable because idling a resource should not increase power.  As a result, application 0 still fufills its purpose of keeping applications from using additional resources that have poor performance/power ratios.



\subsection{Model Update and Downdate Algorithms}

\subsubsection{Row Update and Downdate}

Downdating makes an instructive example. A row downdate operation applies
a sequence of Givens rotations to the rows of $Q^T$.
The rotations are calculated to set every $Q^T_{i,dd}$, $i \neq dd$ to zero.
In the end only the diagonal element $Q^T_{dd,dd}$ of column $dd$ will be nonzero.
Since $Q^T$ remains orthogonal, the non-diagonal elements of row $dd$ will also have been zeroed automatically
and the diagonal element will have absolute value 1.
These same rotations are concurrently applied to the elements of $Q^T t$ and to the rows of $R$ $(= Q^T D)$
to reflect the effect that these transformations have on $Q^T$.

It is crucial to select pairs of rows and an order of rotations that preserves the upper triangular structure of $R$
while zeroing all but the diagonal entry of the chosen column $dd$ of $Q^T$.
Since $dd$ is always below the diagonal of $R$ it initially will contain only zeros.
It is therefore sufficient to rotate every non-$dd$ row with row $dd$, proceeding from bottom to top.
The first $m - n - 1$ rotations will keep row $R_{dd,*}$ entirely zero,
and the remaining $n$ rotations will introduce nonzeros in $R_{dd,*}$ from right to left.
The effect on $R$ will be to replace zero elements by nonzero elements only within row $dd$.
At this point, except for a possible difference in overall sign, $R_{dd,*} = D_{dd,*}$.

Now the rows from 0 down through $dd$ of the modified matrices $Q^Tt$ and $R$ and both the rows and columns of the modified $Q^T$
are circularly shifted by one position, moving row $dd$ to the top (and column $dd$ of $Q^T$ to the left edge).
The following is the result:
\begin{displaymath}
\begin{array}{lll}
    \left[\begin{array}{cc}
      \pm1  &  0 \\
      0     &  \tilde{Q}^T
   \end{array}\right]
   \left[\begin{array}{c}
      t_{dd} \\
      \tilde{t}
   \end{array}\right]
   &=&
   \left[\begin{array}{c}
      \pm D_{dd,*} \\
      \tilde{R}
   \end{array}\right] w
   \\
   \\
   &-&
   \left[\begin{array}{cc}
      \pm1  &  0 \\
      0     &  \tilde{Q}^T
   \end{array}\right]
   \left[\begin{array}{c}
      \varepsilon_{dd} \\
      \tilde{\varepsilon}
   \end{array}\right]
\end{array}
\end{displaymath}
The top row has thus been decoupled from the rest of the factorization and may either be deleted or updated with new data.

The update process more or less reverses these steps, adding a new top row to $R$ and $t$ and a row and column to $Q^T$.
Then $R$ is made upper triangular once more by a sequence of Givens rotations that zero its sub-diagonal elements
(formerly the diagonal elements of $\tilde{R}$) one at a time.
These rotations are applied not just to $R$ but also to $Q^Tt$ and of course to $Q^T$ itself.

\subsubsection{Rank Preservation}

Deciding in advance whether downdating a row of $R$ will reduce its rank
is equivalent to predicting whether one of the Givens rotations, when applied to $R$,
will zero or nearly zero a diagonal entry of $R$.
This is particularly easy to discover because $dd$, the row to be downdated, is initially all zeros in $R$,
\emph{i.e.} in the lower part of the matrix.
In this situation a diagonal entry of $R$, $R_{i,i}$ say, will be compromised if and only if the
cosine of the Givens rotation that involves rows $dd$ and $i$ is nearly zero.
The result will be an interchange of the zero in $R_{dd,i}$ with the nonzero diagonal element $R_{i,i}$.
$R_{dd,i}$ is zero before the rotation because
$R$ was originally upper triangular and prior rotations only involved row subscripts greater than $i$.

\pacora keeps track of the sequence of values in $Q^T_{dd,dd}$ without actually changing $Q^T$
so that if the downdate at location $dd$ is eventually aborted there is nothing to undo.
It is also possible to remember the sines and cosines of the sequence of rotations
so they don't have to be recomputed if success ensues.
A rank-preserving row to downdate will always be available as long as $R$ is sufficiently ``tall''.
Having at least twice as many rows as columns is enough since the number of available rows to downdate
matches or exceeds the maximum possible rank of $R$.

\subsubsection{Column Update and Downdate}

The active-set NNLS method is based on the idea that since the only constraints are variable positivity
then for all components either the variable or its gradient will be zero at a solution point; see~\cite{BoVa}, page~142.
The active set, denoted by \textbf{Z}, comprises the column subscripts $j$ for which the variable $w_j$ is zero and the gradient $v_j$ is positive. If a column $j$ not currently in \textbf{Z} happens to acquire a negative $w_j$ after a back-solve, $w_j$ is zeroed,
$j$ is moved into \textbf{Z} and column $j$ is downdated in $R$, thereby making the gradient positive.
Conversely, if a column already in \textbf{Z} happens to acquire a negative gradient $v_j$ it is removed from \textbf{Z} and updated in $R$,
allowing it to further reduce the value of the objective function.

After initial acquisition of data and $QR$ factorization, each step of \pacora's NNLS algorithm
combines incremental row and column downdates and updates as follows:

\begin{pseudocode}{IncrementalNNLS}{t_0,d_0}
\LOCAL{R,Q^T,Q^Tt,w,v,idx,d,u,done}                              \\
R,Q^T,Q^Tt \GETS \textsc{DndtRow}(R,Q^T,Q^Tt,idx)           \\
R,Q^T,Q^Tt \GETS \textsc{UpdtRow}(t_0,d_0,R,Q^T,Q^Tt,idx)     \\
w \GETS \textsc{BackSolve}(R,Q^Tt,idx)                          \\
v \GETS \textsc{Gradient}(R,Q^Tt,idx)                    \\
\REPEAT
  done \GETS \TRUE                                              \\
  d \GETS \arg\min(w)                                          \\
  \IF w_d < 0 \THEN                                            \\
  \BEGIN
    done \GETS \FALSE                                         \\
    R,Q^T,Q^Tt,idx \GETS \textsc{DndtCol}(R,Q^T,Q^Tt,idx,d)   \\
    w \GETS \textsc{BackSolve}(R,Q^Tt,idx)                    \\
    v \GETS \textsc{Gradient}(R,Q^Tt,idx)              \\
  \END                                                        \\
  u \GETS \arg\min(v)                                         \\
  \IF v_u < 0 \THEN                                           \\
  \BEGIN
    done \GETS \FALSE                                         \\
    R,Q^T,Q^Tt,idx \GETS \textsc{UpdtCol}(R,Q^T,Q^Tt,idx,u)     \\
    w \GETS \textsc{BackSolve}(R,Q^Tt,idx)                    \\
    v \GETS \textsc{Gradient}(R,Q^Tt,idx)              \\
  \END                                                        \\
\UNTIL done                                                   \\
\RETURN{w,v}                                                  \\
\end{pseudocode}

The set \textbf{Z} and its complement \textbf{P} are implemented as an index $idx$
containing a vector of the column subscripts comprising \textbf{P} in increasing order
followed by the column subscripts of \textbf{Z} in increasing order;
$idx$ also contains an offset defining the beginning of \textbf{Z} in the vector.
For example, if columns 1, 3, and 4 are in \textbf{Z} and columns 0, 2, and 5 are in \textbf{P}
then the resulting vector is [0 2 5 1 3 4] and the offset is 3.
Since the offset is just the size of the set \textbf{P} it is naturally called $p$.

Regaqrdless of status, columns are left in place in $R$
The columns of $R$ belonging to \textbf{P} are denoted by $R^p$ and those in \textbf{Z} by $R^z$.
The updating or downdating of a column only involves modifying the index $idx$ to redefine \textbf{P} and \textbf{Z} and then
applying Givens rotations to the rows of $R$ to restore $R^p$ to upper triangular form.

When a column indexed by $d$ in $R^p$ is downdated because $w_d < 0$, that column is moved from \textbf{P} to \textbf{Z} in $idx$.
To restore $R^p$ to upper triangular form, Givens rotations are applied to $R$ at rows $R_{d,*}$ and $R_{k,*}$
where $d < k < p$. The row subscripts $k$ are used in decreasing order from $p-1$ down to $d+1$,
and each rotation zeros the subdiagonal element in $R^p$ of the column indexed by $k$.
As usual, these rotations are also applied to $Q^T$ and $Q^Tt$.
The result in $R^z$ is a ``spike'' of nonzeros in the column that was moved;
it can eventually extend to the bottom of $R$ as \emph{row} updates occur.

Column movements from \textbf{Z} to \textbf{P} are based on the gradient $v$ of the objective function, namely
\begin{eqnarray*}
v &=& 1/2\nabla\|Dw - t\|^2_2 \\
  &=& D^T(Dw - t)             \\
  &=& R^TQ^T(QRw - t)         \\
  &=& R^T(Rw - Q^Tt)          \\
  &=& R^T(-Q^T\varepsilon).
\end{eqnarray*}
If for some column in \textbf{Z} the inner product of the corresponding spiked row in $R^T$ and $-Q^T\varepsilon$ is negative,
the column subscript must be moved to \textsc{P}.
Updating $R^p$ reverses the downdating steps by zeroing the spike via a sequence of Givens rotations on $R$
between adjacent pairs of rows, starting at the bottom and ending at $m,m+1$ where $m$ is the position of the new column in $idx$.
These rotations conveniently extend the columns to the right of $m$ in $R^p$ by one,
thus restoring $R^p$ to upper triangular form. Once again, the rotations are also applied to $Q^T$ and $Q^Tt$.

A new gradient computation and new back-solve for $w$ are clearly necessary after either downdates or updates to columns of $R$.



\section{Dynamic Penalty Optimization}\label{dyn_opt}

%IV.	Dynamic Optimization
%	a.	Gradient Descent w/ Backtracking Search
%		i. boundary conditions clean up
%	b.	Dealing with Fractional Results

\pacora's penalty optimization algorithm dynamically decides resource allocations. The algorithm can be run periodically, when applications start or stop, when an application changes phase or when the system changes operating scenarios.  One of the advantages of convex optimization is that it enables fast, incremental solutions.  As shown in our experiments, the algorithm can terminate earlier to decrease overhead and still be moving towards an optimal solution as it runs.  %However we found in our implementation that the algorithm was fast enough to run to completion every time. 

Convex optimization is simplest when it is unconstrained, so we reformulated \pacora's construction to be unconstrained.
Extending the response time model functions to all of $\Re^n$
moves the requirement that allocations must be positive into the objective function,
and introducing application 0 for slack resources turns the affine inequalities into equalities:
\begin{eqnarray*}
& \makebox[1in][r]{Minimize}   & \sum_{p\in P} {\pi_p(\tau_p(a_{p,1}\ldots a_{p,n}))}  \\
& \makebox[1in][r]{Subject to} & \sum_{p\in P} a_{p,r} = A_r, r = 1,\ldots n           \\
\end{eqnarray*}

The only remaining constraints are those on the $a_{p,r}$.
These can be removed by letting the $a_{p,r}$ be unbounded above for $p \neq 0$
and changing the domain of $\tau_0$  to be the whole resource allocation matrix.
The definition of $\tau_0$ might take the form
\begin{eqnarray*}
\tau_0 &=& \sum_r \Delta_r \sum_{p \neq 0} a_{p,r}     \\
       &=& \sum_r \Delta_r (A_r - a_{0,r})
\end{eqnarray*}
where $\Delta_r$ is the (constant) power dissipation of one unit of resource $r$.
However, if any of the allocations $a_{0,r}$ turns out to be negative then $\tau_0$  should instead return the value $+\infty$.
%This modification of the objective function transforms the resource allocation problem
%to unconstrained convex optimization.  

The penalty optimization algorithm used in \pacora is gradient descent via backtracking line search along the negative gradient direction \cite{BoVa}.
This algorithm rejects and refines any step that yields insufficient relative improvement in the objective function,
so infinite values from infeasible allocations will automatically be avoided by the search.
The negative gradient $-\nabla\pi$ of the overall objective function $\pi$
with respect to the resource allocations $a$
is computed analytically from the response time models and penalty functions.
When a component of this overall gradient is negative,
it means the penalty will be reduced by increasing the associated allocation if possible.
The gradient search at the boundaries of the feasible region
must ignore components that lead in infeasible directions;
these can be detected by noting whether for some $p$ and $r$, $a_{p,r} = 0$ with $(-\nabla\pi)_{p,r} > 0$.
In such cases, the associated step component is set to zero.

We added an additional optimization to move along boundaries more rapidly in the scenario when a completely allocated resource had a large gradient.  We scale all the allocations of that resource type down to satisfy resource constraint while leaving the allocations of other resources untouched.

The rate of convergence of gradient descent depends on how well the sub-level sets of the objective function
are conditioned (basically, how ``spherical'' they are).
Conditioning will improve if resource allocation units are scaled to make their relative effects similar.
For example, when compared with processor allocation units,
memory allocation units of 4 MB are probably a better choice than 4 KB.
In addition, penalty function slopes should not differ by more than perhaps two orders of magnitude. If these measures prove insufficient, stronger preconditioners can be used. Our implementation conditions all resource allocations to be in the range of 0-100.

%\begin{figure}[!t]
%	\begin{center}	
%		\includegraphics[bb=0 0 576 432,width=\columnwidth]{opt_time.pdf}
%%		\includegraphics[width=.45\textwidth]{parsec_decision_points.pdf}
%		\caption{Performance of our penalty optimization algorithm}
%		\label{optimization_perf}
%	\end{center}
%\end{figure}


%Figure~\ref{optimization_perf} shows performance of the penalty optimization algorithm implemented in \tess.  

%For our video conferencing scenario the average runtime is \fix{x} and then worst case runtime is \fix{y}.  \fix{explain where the variance comes from}


\subsection{Resource allocation}

We follow the notation in \S 7.3 of Boyd et al. \cite{ADMM},
from which this material is taken almost verbatim.
We have $n$ resources and $N$ participants.
We let $x_i \in \reals^n_+$ denote the vector of resources 
that participant $i$ consumes.  
The (vector of) total resource consumption is then $x_1 + \cdots + x_N$;
for future use we let $z$ denote the average resource
usage per participant, \ie, the total divided by $N$.
Participant $i$ has a cost function $f_i:\reals^n \to \reals \cup
\{\infty\}$, where
$f_i$ is convex.  We let $f_i$ take on the value $+\infty$ to encode
constraints on the resource allocation.

The total cost function is
\[
f_1(x_1) + \cdots + f_N (x_N) + g(Nz),
\]
where $g: \reals^n \to \reals \cup \{+\infty\}$ is the cost
of consuming a total amount of resources (including any limits on total
available resources).
Note that the first $N$ terms are the costs associated with the
participants, and the last term is the cost of providing the total
resources.
The problem is to choose the allocations $x_i$ to minimize the total cost,
which is a convex optimization problem \cite{BoVa,ADMM}.

We will solve this problem using the \emph{sharing ADMM algorithm}
from \cite{ADMM}:
\begin{eqnarray*}
x_i^{k+1} &:=&  \argmin_{x_i} \left( f_i(x_i) + (\rho/2)
\|x_i-x_i^k+\overline x^k-z^k+u^k\|^2_2\right)\\
z^{k+1} &:=&  \argmin_{z} \left( g(Nz) + (N\rho/2)
\|z - u^k - \overline x^{k+1} \|_2^2 \right)\\
u^{k+1} &:=&  u^k + \overline x^{k+1}- z^{k+1}.
\end{eqnarray*}
Here $\rho>0$ is an algorithm parameter,
$k$ is the iteration number,
and $\overline x^k$ is the average of the consumption vectors
$x^k_1, \ldots, x^k_N$.
We interpret $x_i^k$ as the (proposed) resource consumption of 
participant $i$,
$z^k$ as the (proposed) average resource consumption,
and $u^k$ as a dual variable, all at iteration $k$.
This algorithm converges to an optimal allocation,
and $(1/\rho)u^k$ converges
to the optimal dual variables (prices) for the resources.

The $x$-update can be carried out in parallel, for $i=1, \ldots, N$.
Each participant, in each iteration, must minimize a function of the form
\[
f_i(x_i) + (\rho/2) \|x_i - v\|_2^2,
\]
\ie, each participant evaluates a proximal operator; see
\cite{ProxAlgs}.

The $z$-update step requires
gathering $x_i^{k+1}$ to form the averages, and then solving a problem
with $n$ variables.  This step is also a proximal evaluation.

After the $u$-update, the new value of $\overline x^{k+1}-
z^{k+1}+u^{k+1}$ is scattered to the subsystems.

\subsection{Excess latency minimization}

We now specialize to a latency minimization problem,
where the participants are processes and the participant costs 
are related to their latencies.

\subsubsection{Participants}

The latency (time delay) of each process depends on the 
resources allocated to the process.
Our model of the latency of process $i$ is
\[
l_i(x_i) = a_i + \sum_{j=1}^n (w_i)_j/(x_i)_j
\]
for $x_i >0$, and $+\infty$ otherwise, where $a_i \in \reals_+$ and 
$w_i \in \reals_{++}^n$ are given latency model parameters.
The participant cost is given by
\[
f_i(x_i) = \alpha_i (l_i(x_i) - t_i)_+,
\]
where $\alpha >0$ is a parameter and $t_i$ is a target latency.
Note that $l_i(x_i)-t_i$ is the excess latency.

We commment that the target latency in the cost function here is
probably not the actual target latency for our system, but rather
a target latency below which we do not impose a cost.  If the system
specification is for process $i$ to have (to the extent possible) latency
not exceeding $T_i$, then we might choose, for example, $t_i= 0.9T_i$.
In this case, there is no latency cost as long as the process latency is
less than the specification latency, with at least $10\%$ margin.

In each iteration of the sharing algorithm, 
we need to evaluate the proximal operator of $f_i$.
To simply notation, we drop the subscript $i$ and consider one
participant.
We need to minimize
\[
\alpha \left(a + \sum_{j=1}^n w_j/x_j - t\right)_+ + 
(\rho/2)\sum_{j=1}^n (x_j - v_j)^2
\]
over $x_j\geq 0$.
Note that we can combine $\alpha$ and $\rho$ (say, by dividing by 
$\alpha$) and we can combine $a$ and $t$.  So we now 
assume that $\alpha =1$ and $a=0$, with the understanding that
$\alpha$ and $a$ have been incorporated into $\rho$ and $t$.

We work out several cases.  First suppose that the excess latency
is negative.  
The first term above is zero and we must have $x=v$.  So $x=v$ is
the solution when $v>0$ and
\[
\sum_{j=1}^n w_j/v_j - t \leq 0.
\]

Now consider the case when the excess latency is positive.  In this case
we simply minimize
\[
\sum_{j=1}^n w_j/x_j +
(\rho/2)\sum_{j=1}^n (x_j - v_j)^2,
\]
which can be done for each $x_j$ separately.
Each $x_j$ must satisfy
\[
w_j/x_j^2  = \rho(x_j - v_j).
\]
This can be solved extremely quickly, using a bisection method, Newton's
method, or many others to find the unique (positive) 
values $x_j^\star$ that satisfy
the equation.
We then check if the resulting values of $x_j$ give nonnegative
excess latency, \ie, if 
\[
\sum_{j=1}^n w_j/x_j^\star - t  \geq 0.
\]
If so, we are done: $x^\star$ is the value of the proximal operator.

Finally, we consider the special case (which occurs often) when
the optimal values have zero excess latency, \ie,
\[
\sum_{j=1}^n w_j/x_j - t  = 0.
\]
The optimality condition in this case is that exists a $\theta \in [0,1]$
for which
\[
\theta w_j/x_j^2 = \rho(x_j - v_j)
\]
(along with the condition $\sum_{j=1}^n w_j/x_j - t  = 0$).
This we solve by bisection on $\theta$.  For each value of $\theta$,
we use the same method as above to find $x_j$.  We then check if
$\sum_{j=1}^n w_j/x_j - t  = 0$ is positive or negative.
If it is positive, we increase $\theta$; otherwise we decrease it.
(We could also apply a Newton method to solve the two equations for
$x_j$ and $\theta$.)

\paragraph{Summary.} 
We can write a little C code that computes the proximal operator
for each participant very fast.
A naive implementation will be fast; an optimized one (say,
in whcih we precompute the solution with a lookup table) will be very very
fast.

\subsubsection{Total resource cost}

We take the resource cost to have the form
\[
g(z) = \sum_{i=1}^n g_i(z_i).
\]
Here $g_i(z_i)$ is the cost of providing resource $i$ at level $z_i$.
A simple model is 
\[
g_i(z_i) = \left\{ \begin{array}{ll} 
c_i z_i & z_i \leq Z_i\\
+ \infty & z_i > Z_i ~\mbox{or}~z_i<0,
\end{array} \right.
\]
where $Z_i>0$ is the maximum available, and $c_i>0$ is the price for
resource $i$.
Since $g$ is separable, we can minimize over each resource 
separately; these are scalar problems.

We need to minimize 
\[
g_i(Nz_i)  +(N\rho/2)(z_i - v_i)^2
\]
over the (scalar) $z_i$. (Here $v_i = u^k_i + \overline x^{k+1}_i$.)
The solution is simple:
\[
    z_i = \max \{0, \min \{ v_i - c_i/\rho, Z_i/N\} \}.
\]
Note that $N$ drops out, but we need to scale the bound accordingly.
Also note that when the average $z_i = Z_i/N$, the total amount of
resource is $N z_i=Z_i$, meaning 
that resource $i$ is at its maximum possible level.

%\paragraph{Summary.} We can write a few lines of C code that carries out
%the $z$ update step.  It will be exceedingly fast.

\subsubsection{Total resource cost with free zone}
We model the resource cost in terms of energy consumed with a function 
of the form
\[
    g(z) = \lambda \left(\sum_{i=1}^n c_i z_i - b\right)_+,
\]
where $c_i>0$ represents the amount of energy consumed by
unit amount of resource~$i$, 
the constant~$b>0$ is a threshold below which power consumption is free,
and~$\lambda>0$ is the price charged for excess energy used
(or relative weight used to tradeoff between latency and energy).
We also impose lower and upper bounds on~$z$, \ie, $0\leq z_i\leq Z_i$
for $i=1,\ldots, n$.

To evaluate the proximal operator, we need to minimize
\[
    g(Nz) + \frac{N\rho}{2}\sum_{i=1}^n (z_i-v_i)^2,
\]
where $z$ is the averge resource vector, and
$v_i = u^k_i + \overline x^{k+1}_i$.
This is equivalent to
\[
\begin{array}{ll}
\mbox{minimize} &  \lambda\left(\sum_{i=1}^n c_i z_i - b/N\right)_+
     + (\rho/2)\sum_{i=1}^n (z_i-v_i)^2 \\
\mbox{subject to} & 0\leq z_i \leq Z_i/N, \quad i=1,\ldots,n.
\end{array}
\]
The solution can be obtained in a way similar to that used for evaluating 
the proximal operator of the latency penalty functions.

We work out several cases. 
First suppose that the excess energy is negative. 
The first term in the objective function is zero. 
So the solution is
\[
    z_i = \max\{0, \min\{v_i, Z_i/N\}\}, \quad i=1,\ldots, n
\]
provided that
\[
    \sum_{i=1}^n c_i z_i - b/N \leq 0.
\]

Next we consider the case when the excess energy is positive.
In this case, we simply minimize
\[
    \lambda \left(\sum_{i=1}^n c_i z_i - b/N \right) 
    + (\rho/2)\sum_{i=1}^n (z_i-v_i)^2,
\]
which can be solved for each $z_i$ separately and the solutions are
\[
    z_i = \max\{0, \min\{v_i - \lambda c_i/\rho, Z_i/N\}\}, \quad i=1,\ldots, n.
\]
We then need to check
\[
    \sum_{i=1}^n c_i z_i - b/N \geq 0.
\]
If so, we are done.

Finally we consider the case when the optimal allocations have zero excess
enerty, \ie,
\[
    \sum_{i=1}^n c_i z_i - b/N = 0.
\]
The solution takes the form
\[
    z_i = \max\{0, \min\{v_i - \theta c_i/\rho, Z_i/N\}\}, \quad i=1,\ldots, n.
\]
where $0\leq\theta\leq\lambda$.
We can do a bisection on $\theta$ to make the solution satisfy
$\sum_{i=1}^n c_i z_i-b/N=0$.
Basically, if the excess energy is positive, then we increase~$\theta$;
otherwise we decrease it.




\subsubsection{Smoothing regularization}

Suppose we need to do resource allocation repeatedly over time.
We can add a term of the form
\[
\mu \| x_i - x_i^\mathrm{prev}\|_1,
\]
where $\mu > 0$ is a parameter, to the latency cost function.
This adds a cost for changing the resource allocation from its
previous value.
The larger $\mu$ is, the less frequently the resource allocations change
over time (which comes at the cost of higher objective cost).
As an extension, we can have different values of $\mu_i$ for each
resource; this allows some resource levels to be adjusted more often than
others.

The addition of smoothing regularization would require a few 
changes to the proximal operator evaluation described above.

\subsection{Real-time resource allocation}

Here we describe how the sharing algorithm is used in a real-time 
resource allocation system.
Resources are (possibly) re-allocated in time epochs.
In each time epoch, we (possibly) 
(re-)estimate the latency model parameters $a_i$, $w_i$, and
obtain (possibly) updated values for the resource usage cost 
parameters $c_i$ and $Z_i$.
These new values are used for the next round of sharing algorithm iteration,
starting from the values in the previous time epoch.  (This is called
\emph{warm start}.
The converged values are then used for the allocations in the 
current epoch.

\subsection{implementation details}

\subsubsection{Stopping criteria}
Here we describe a stopping criterion that is similar to the one
in \S3.3 of \cite{ADMM} applied the resource allocatoin problem.

For our resource allocation problem, the primal residuals at iteration~$k$ are
\[
    r_i^k = x_i^k - z_i^k, \quad i=1,\ldots,N,
\]
and the dual residuals are
\[
    s_i^k = \rho (z_i^{k-1}-z_i^{k}), \quad i=1,\ldots,N.
\]
Here $z_i$ for $i=1,\ldots, N$ are the variables that were eliminated
to simplify the $z$-update in the sharing problem 
(see \S7.3 of \cite{ADMM}).
The variable~$z$ in the  simplified update is actually 
their average $\overline z=(1/N)\sum_{i=1}^N z_i$.
Based on the derivation in \cite[\S7.3]{ADMM}, 
\[
    z_i^k = x_i^k -\overline x^k + \overline z^k , \quad i=1,\ldots,N.
\]
Therefore we have
\[
    r_i^k = x_i^k - z_i^k = \overline x^k - \overline z^k, \quad i=1,\ldots,N,
\]
\ie, the primal residuals for the processes are the same as the average
primal residue.
The dual residuals become
\begin{eqnarray*}
    s_i^k &=& \rho(z_i^{k-1} - z_i^k) \\
    &=&\rho\bigl( (x_i^{k-1}-x_i^k) +(\overline x^k-\overline z^k)
    -(\overline x^{k-1} - \overline z^{k-1})\bigr)\\
    &=& \rho\bigl( (x_i^{k-1}-x_i^k) + r^k - r^{k-1} \bigr),
    \quad i=1,\ldots, N.
\end{eqnarray*}
So the dual residuals are different for different processes.

The following termination criterion is similar to the one proposed in
\cite[\S3.3]{ADMM}:
\begin{eqnarray*}
\|r^k\|_2 = \|\overline x^k - \overline z^k\|_2 &\leq& \epsilon^\mathrm{pri},\\
\max \{\|s_1^k\|_2,\ldots,\|s_N^k\|_2 \} &\leq& \epsilon^\mathrm{dual},
\end{eqnarray*}
with
\begin{eqnarray*}
\epsilon^\mathrm{pri} &=& \sqrt{n}\epsilon^\mathrm{abs} + \epsilon^\mathrm{rel}
    \max\{\|x_1\|_2,\ldots,\|x_N\|_2,\|z_1\|_2,\ldots,\|z_N\|_2\} ,\\
\epsilon^\mathrm{dual} &=& \sqrt{n}\epsilon^\mathrm{abs} +\epsilon^\mathrm{rel}
    \rho\|u^k\|_2.
\end{eqnarray*}


Since the computation involved in the above stopping criterion are 
rather heavy, and we also experimented with simplified conditions.
In particular, we tried the following conditions 
which only uses the average vectors:
\begin{eqnarray*}
\|r^k\|_2=\|\overline x^k - \overline z^k\|_2 
&\leq& \sqrt{n}\epsilon^\mathrm{abs}+\epsilon^\mathrm{rel}\|\overline z^k\|_2,\\
\|s^k\|_2 = \rho(\overline z^{k-1}-\overline z^k\|_2)
&\leq& \sqrt{n}\epsilon^\mathrm{abs} +\epsilon^\mathrm{rel}\rho \|u^k\|_2 .
\end{eqnarray*}
Basically we simplified the calculation of $\epsilon^\mathrm{pri}$ and
the dual residual, while leave the calculation of primal residual and
$\epsilon^\mathrm{dual}$ unchanged.

Figure~\ref{fig:residual-e} shows both the accurate calculations and their
simplified counterparts.
We see that the simple primal $\epsilon^\mathrm{pri}$ is slightly smaller
than the more accurate calculation, making primal residual a little harder to
satisfy the termination condition. 
On the other hand, the simple dual residual calculation is smaller than
the accurate dual residual calculation, making thedual residual easier to
satisfy the termination condition.
Figure~\ref{fig:allocation-e} illustrate the optimal resource allocation 
and resulting latency for each of the processes.

It looks that the simplified stopping criterion is effective and 
sufficient.
However, when we vary the parameters in the resource allocation problem, 
the simple conditions may breakdown.
Figure~\ref{fig:residual-c} and~\ref{fig:allocation-c} 
plot the same quantities but in the case of cheap energy (meaning that
price $\lambda$ in the total resource cost function is small).
In this case, the total resouces allocated all reach their bounds, 
and most of the process latency are within their deadlines.
Notice that the simplified dual residual becomes exactly zero 
(discontinued in the right plot of Figure~\ref{fig:residual-c}) 
after 10 iterations.
The reason is that since the enerygy is cheap, the resource allocations 
reach their bounds easily,
so the simple dual residual $\|s^k\|_2=\rho\|z^{k}-z^{k+1}\|_2=0$ 
become zero quickly, therefore can \emph{not} serve as a stopping criterion.

In our implementation, we use the simplfied calculation of 
$\epsilon^\mathrm{pri}$,
but do not simplify the calculation of the dual residual.

\begin{figure}[th]
\centering
\includegraphics[width=0.49\textwidth]{figures/test_primal_e.eps}
\includegraphics[width=0.49\textwidth]{figures/test_dual_e.eps}
\caption{Progress of reducing primal and dual residual norms in ADMM.
    This is the case of expensive energy, notice that the simple dual
residual often work as well as the accurate dual residual. Since
the resource allocations are not reaching their bounds, so the simple
dual residual $\|s^k\|_2=\rho\|z^{k}-z^{k+1}\|_2$ converges to zero 
only asymptotically, and can serve as a stopping criterion.}
\label{fig:residual-e}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=0.49\textwidth]{figures/test_resource_e.eps}
\includegraphics[width=0.49\textwidth]{figures/test_latency_e.eps}
\caption{Visualization of optimal resource allocation (left) and 
    resulting latency for each process (right).
    There are $n=10$ resources and $N=20$ processes.
    This is the case of expensive energy, so the total resouces allocated
    are mostly well below their bounds, but the process latency are mostly
exceeding the deadlines.}
\label{fig:allocation-e}
\end{figure}


\begin{figure}[th]
\centering
\includegraphics[width=0.49\textwidth]{figures/test_primal_c.eps}
\includegraphics[width=0.49\textwidth]{figures/test_dual_c.eps}
\caption{Progress of reducing primal and dual residual norms in ADMM.
    This is the case of cheap energy, notice that the simple dual
residual becomes exactly zero (discontinued in the plot) after 10 iterations, 
Since the enerygy is cheap, the resource allocations reach their bounds easily,
so the simple dual residual $\|s^k\|_2=\rho\|z^{k}-z^{k+1}\|_2=0$ 
become zero quickly, therefore can \emph{not} serve as a stopping criterion.}
\label{fig:residual-c}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=0.49\textwidth]{figures/test_resource_c.eps}
\includegraphics[width=0.49\textwidth]{figures/test_latency_c.eps}
\caption{Visualization of optimal resource allocation (left) and 
    resulting latency for each process (right).
    There are $n=10$ resources and $N=20$ processes.
    This is the case of cheap energy, so the total resouces allocated
    all reach their bounds, and most of the process latency are 
within their deadlines.}
\label{fig:allocation-c}
\end{figure}

