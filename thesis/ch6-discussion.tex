\chapter{Discussion}\label{discuss}

don't forget to discuss, averaging response times, modifying applications, measuring heartbeats, inferring penalty or deadlines

There are two main sources of challenges for \pacora's design: performance non-convexity and performance variability.  In the following section we describe possible techniques for coping with these challenges.

The main concern with performance non-convexity and variability is their effects on the accuracy of the response time functions.  However, an important result we have found while evaluating \pacora is that model accuracy has less impact on the quality of resource allocation decisions than we anticipated.  When experimenting with possible models for the RTFs, we found that while some models were always a little too inaccurate and did degrade the performance of the resource allocation decisions, once a model crossed a certain threshold of accuracy then better models provided insignificant improvement in resource allocation decisions.  Although there is a slight correlation between model accuracy and decision quality, many decisions with inaccurate models still result in near optimal allocations.  This effect enables \pacora's model-based design to be feasible in a noisy system with real applications.

\section{Outliers and Performance Non-Convexity}

Outliers and performance non-convexity can be handled with a combination of two techniques.  Outliers should be thrown out during the model creation phase as described in Section~\ref{RTFs} to prevent those points from distorting the accuracy of the other allocations.  The second part of the solution is to keep track of these points with extreme error in the model and use heuristics to adjust the resource allocations to avoid such points.  We did not implement this second idea in our current evaluation, but it is a subject of future study.  In our study, we experienced outliers for a few applications with a particular number of hyperthreads and for many applications when given only one cache way. However, as responsiveness, predictability, and efficiency increase in importance for systems, we expect to see an increased number of chip designs that provide more performance convexity.

\section{Variability}

We observe three main sources of variability in application performance: phase changes, performance changes due to differing inputs, and variable resource performance due to external causes or interference from sharing.

\section*{Phases}
As described in Section~\ref{RTFs}, phases can be handled with online creation of the RTFs.  However, another option is to build different models, one per phase, and swap models when a change occurs.  We used this method for the video application in Section~\ref{eval-dynamic}.  This approach has the advantage that it can more rapidly adapt to phases; however it requires identifying phases and additional space to store the extra models. Phase detection is an active area of research, and \cite{dhodapkar-micro03} provides an overview of techniques.  Another possible approach is to build a model that represents the resource requirements of the most demanding phase.  The system can be designed make use of the idle resources when available or power management mechanisms can put them in low-power mode.

\section*{Input Dependence}
Some applications may significantly change performance as a function of their inputs. In the case of our video application we ignored its input dependence without significant effect.  However for other applications the effect may be more pronounced. As with phases, a solution might be to keep multiple models for the application and select one based on the current input.  A different solution is to make the RTFs \emph{stochastic models} representing the distribution of response times of the application.  Stochastic models for \pacora are discussed in more detail in~\cite{pacora_tr}.

\section*{Resource Variability}
In our study we experience resource variability both in the form of non-deterministic resources such as the network connection in \texttt{ tradebeans} or dynamic frequency changes in the CPU and shared resources such as memory bandwidth. Stochastic models are most likely the right way to deal with non-deterministic resources and may be particularly necessary for representing disk-based storage in warehouse scale computing.

Shared resources can also be handled with stochastic models.  However, since the models can be built online while other applications are running, the interference from a loaded machine is already captured in the model.  In our evaluation of static allocation, we built the models in isolation but found that \pacora was still able to make near-optimal resource allocations for a loaded machine despite the shared resources.

While there will likely always be some shared resources, we have found
a trend towards minimizing interference in emerging chip designs, as
efficiency and predictability begin to trump utilization as primary
concerns.


%other types of non-deterministic apps?



% don't forget cpu frequency

%------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Possible RTF Extensions}
%------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Alternative Resource Representations}
Asynchrony and latency tolerance may make response time components overlap partly or fully;
if the latter, then the maximum of the terms might be more appropriate than their sum.
The result will still be convex, though, as will any other norm including the 2-norm,
\emph{i.e.} the square root of the sum of the squares.
This last variation could be viewed as a ``partially overlapped'' compromise between
the 1-norm (sum) describing no overlap and the $\infty$-norm (maximum) describing full overlap.

Sometimes a response time component might be better modeled by a term involving a combination of resources.
For example, response time due to memory accesses might be approximated
by a combination of memory bandwidth allocation $b_{r1}$ and cache allocation $m_{r2}$.
Such a model could use the geometric mean of the two allocations in the denominator,
\emph{viz.} $w_{r1,r2}/\sqrt{b_{r1}\cdot m_{r2}}$, without compromising convexity.

This scheme also accommodates non-bandwidth resources such as memory,
the general idea being to roughly approximate ``diminishing returns'' in the response time with increasing resources.
For clarity's sake, rather than using $a_r$ indiscriminately for all allocations,
we will denote an allocation of a bandwidth resource by $b_r$ and of a memory resource by $m_r$.
This begs the question of how memory affects the response time.
The effect is largely indirect.
Memory permits exploitation of temporal locality and thereby \emph{amplifies} associated bandwidths.
For example, additional main memory may reduce the need for storage or network bandwidth,
and of course increased cache capacity may reduce the need for memory bandwidth.
The effectiveness of cache in reducing bandwidth was studied by
H. T. Kung\cite{Kung}, who developed tight asymptotic bounds on the bandwidth amplification
factor $\alpha(m)$ resulting from a quantity of memory $m$ acting as cache for a variety of computations.
He shows that
\begin{displaymath}
\begin{array}{lll}
\alpha(m) &= \Theta(\sqrt m) & \mbox{for dense linear algebra solvers} \\
          &= \Theta(m^{1/d}) & \mbox{for d-dimensional PDE solvers} \\
          &= \Theta(\log m)  & \mbox{for comparison sorting and FFTs} \\
          &= \Theta(1)       & \mbox{when temporal locality is absent}
\end{array}
\end{displaymath}

For these expressions to make sense, the argument of $\alpha$ should be dimensionless and greater than 1.
Ensuring this might be as simple as letting it be the number of memory resource quanta
(\emph{e.g.} hundreds of memory pages) assigned to the application.
If a application shows diminishing bandwidth amplification as its memory allocation increases, this can be accommodated:
\begin{displaymath}
\alpha(m) = \min(c_1\alpha_1(m),c_2\alpha_2(m)),\;c_1,c_2 \geq 0
\end{displaymath}

Each bandwidth amplification factor might be described by one of the functions above
and included in the denominator of the appropriate component in the response time function model.
For example, the storage response time component for the model of an out-of-core sort application might be
the quantity of storage accesses divided by the product of the storage bandwidth allocation and $\log m$,
the amplification function associated with sorting given a memory allocation of $m$.
Amplification functions for each application might be learned from response time measurements
by observing the effect of varying the associated memory resource while keeping the bandwidth allocation constant.
Alternatively, redundant components, similar except for amplification function, could be included in the model
to let the model fitting process decide among them.

\subsection{Stochastic Models}
Execution times are random to some extent. Variations in amounts of work within an application are reflected only after the fact by the $w_i$ which change as the process runs.  Insufficiently isolated resources may also cause such variability. Complicating things further, service objectives may specify guarantees in the form of high probability upper bounds on actual application runtimes.  The predicted runtimes $\tau$ in these cases are insufficient.
Random fluctuations will be reflected in the runtime measurements t and the residual error $\epsilon = t - \sigma$. The sample mean and standard deviation of the error can bound the probability that the actual runtime will exceed the prediction $\tau$. Using this information, a Chebyshev bound on the error of the form
\begin{equation}
Pr\left\{\left|\frac{(\epsilon-\overline{\epsilon})}{\sigma_\epsilon}\right| > k\right\}\leq\frac{1}{k^2} 
\end{equation}
can guarantee the probability that a particular runtime requirement is met.
For example, a runtime upper bound of 50 microseconds with probability 0.99, given an error sample mean of 3 microseconds (meaning $\tau$ underestimates t by that much) and a sample standard deviation of 2 microseconds, leads to the requirement that $\tau + 3$ must be less than $50 - k \sigma\epsilon = 50 - 10\times2 = 30$ microseconds to guarantee that $t = \tau  + \epsilon$ exceeds 50 microseconds only 1\% of the time.  The penalty function deadline for this application should accordingly be set to 27 microseconds.  Of course if the system is incapable of meeting all of its deadlines, penalty slopes will determine the extent of the violations.




\subsection{Handling Quasiconvex Response Time Functions}

There are examples of response time versus resource behavior that violate convexity.  One such example sometimes occurs in memory allocation, where ``plateaus'' can sometimes be seen as in Figure~\ref{f:plat}.

Such plateaus are typically caused by algorithm adaptations within the application to accommodate variable resource availability.  The response time is really the \emph{minimum} of several convex functions depending on allocation and the point-wise minimum that the application implements fails to preserve convexity.  The effect of the plateaus will be a non-convex penalty as shown in Figure~\ref{f:plateffect} and multiple extrema in the optimization problem will be a likely result.

There are several ways to avoid this problem.  One is based on the observation that such response time functions
will at least be \emph{quasiconvex}.  A function $f$ is quasiconvex if all of its \emph{sublevel sets}
$S_\ell = \{x | f(x) \leq \ell\}$ are convex sets.
Alternatively, $f$ is quasiconvex if its domain is convex and
\begin{displaymath}
f(\theta x + (1-\theta)y) \leq \max(f(x),f(y)), 0 \leq \theta \leq 1
\end{displaymath}

Quasiconvex optimization can be performed by selecting a threshold $\ell$ and replacing the objective function
with a convex constraint function whose sublevel set $S_\ell$ is the same as that of $f$.
Next, one determines whether there is a feasible solution for that particular threshold $\ell$.
Repeated application with a binary search on $\ell$ will reduce the level of feasibility
until the solution is approximated well enough.

Another idea is to use additional constraints to explore convex sub-domains of $\tau$.
For example,the affine constraint $a_{p,r} - \mu \leq 0$ excludes application $p$ from any assignment of resource $r$ exceeding $\mu$.  Similarly, $\mu - a_{p,r} \leq 0$ excludes the opposite possibility.
A binary (or even linear)search of such sub-domains could be used to find the optimal value.


\subsection{Heterogeneity}

Heterogeneity is naturally handled by \pacora.  Each core type can be viewed as a different resource system, so fat cores may be resource 1, thin cores may be resource 2, and GPUs could be resource 3.  The application developer would not need to specify, which core types the application uses best; \pacora would try allocating the cores and the resulting performance would be captured in the RTFs.  So, for example, if an application did not use a GPU then this would be discovered empirically and the RTF would show no performance improvement along the GPU dimension.  









