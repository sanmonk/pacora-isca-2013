\chapter{Related Work}
\section{Batch Scheduling and Cluster Management}
Classic resource management systems were designed for batch scheduling~\cite{Feit97,FRS04}. Like \pacora, batch scheduling systems can allocate multiple resource types and rely on a gang-scheduling model~\cite{FeRu92}.  However, they tend to use queues and priorities to schedule jobs while trying to keep all resources busy. Resource allocations are always the user's responsibility to specify and pay for. Responsiveness can be improved by buying a higher priority. Few batch systems incorporate deadlines, but one such is \cite{AKKMS95}.

Modern cluster resource management has a similar flavor.  In systems such as
Amazon EC2~\cite{EC2}, Eucalyptus~\cite{eucalyptus}, and Condor~\cite{Condor}, users must specify their resource requirements.  Other systems such as Hadoop ~\cite{hadoop_fair, hadoop_cap, hadoop_matei} and Quincy~\cite{Quincy} use a fairness policy to assign resources. Mesos~\cite{mesos} uses two-level scheduling to manage resources in a cluster. Mesos decides how many resources to offer to its applications; they decide which resources to accept and how to schedule them. Mesos does not provide a particular resource allocation policy, but is a framework that can support multiple policies. Dominant Resource Fairness~\cite{mesos-DRF}, a generalization of max-min fairness to multiple resources types, has been implemented in Mesos.

\section{Model-Based Scheduling and Allocation Frameworks for SLAs and Soft Real-Time Requirements}

Research is growing in the cloud computing and realtime communities, which use application-specific performance ``models'' to try to schedule to meet deadlines or SLAs.

Much of this research has been in autonomic computing~\cite{1078472,1078493,1285843,1345325}. Typically, the performance models are utility functions derived from off-line measurements of raw resources utilization. These functions are either interpolations from tables or analytic functions based on queueing theory.
%The utility functions typically map the number of servers each execution environment receives to its performance relative to its requirements, which may be several.
A central arbiter maximizes total utility. The utility functions are not necessarily concave,
so the arbiter must use reinforcement learning or combinatorial search to make allocations.
%Each application has a manager that schedules the resources given to it by the arbiter.
Walsh \emph{et al.}\cite{1078411} note the importance of basing utility functions
on the metrics in which QoS is expressed rather than on the raw quantities of resources.
There are other philosophical similarities to \pacora, but since the objective functions are discrete and non-convex their optimization is difficult. A survey of autonomic systems research appears in \cite{1380585}.

Rajkumar \emph{et al.}\cite{828990} propose a system Q-RAM that maximizes the weighted sum of utility functions, each of which is a function of the resource allocation to the associated application. Unlike \pacora, there is no distinction between performance and utility, and the utility functions are assumed as input rather than being discovered by the system. The functions are sometimes concave, and in these cases the optimal allocation is easily found by a form of gradient ascent. When the utility functions are not concave, a suboptimal greedy algorithm is proposed.
%They are always nondecreasing in every resource and sometimes are piecewise-linear and concave;

Several systems use a feedback-driven reactive approach to resource allocation where a control loop or reinforcement learning adjusts allocations continuously. AcOS\cite{AcOS} and Metronome\cite{Metronome} feature hardware-thread based maintenance of ``heart rate'' targets using adaptive reinforcement learning.
%AcOS also senses thermal conditions and can exploit Dynamic Voltage and Frequency Scaling (DVFS).
Bodik \emph{et al.}\cite{bodik-acdc09} builds online performance models like \pacora.
Initially, it uses an \emph{exploration policy} that avoids nearly all SLA violations while it builds the model; later, it shifts to a controller based on the model it has built.
The models are statistical, and bootstrapping is used to estimate performance variance.
Major changes in the application model are detected and cause model exploration to resume.
%The models are not convex or concave in general, and all SLAs must be met with high probability.

Jockey\cite{Jockey} has some similarities to \pacora: it is intended to handle parallel computation, its utility functions are concave,
and it adapts dynamically to application behavior.
Its performance models are obtained by calibrating either event-based simulation or a version of Amdahl's Law to computations.
Jockey does not optimize total utility but simply increases processors until utility flattens for each application,
\emph{i.e.,} each deadline is met.
A fairly sophisticated control loop prevents oscillatory behavior.

Other systems base decisions on user-provided resource specifications and a real-time scheduling algorithm.  In the Redline system\cite{Redline}, compact resource requirement specifications written by hand to guarantee response times are scheduled Earliest-Deadline-First.
%Isolation of resources is strong, as in \pacora.  Scheduling is Earliest-Deadline-First.
%Admission control is lenient but oversubscription situations are remedied by de-admitting some of the non-interactive applications.

Much research focuses on measuring resource usage to make smart co-scheduling decisions.
For example, Calandrino \emph{et al.}\cite{unc} uses working set sizes to make co-scheduling decisions and enhance soft real-time behavior. Merkel and Bellosa\cite{merkel-eurosys08} propose \emph{Task Activity Vectors} that describe how much each application uses the various functional units; these vectors are used to balance usage across multiple cores.
%and unbalance usage among hardware threads within each core.
%The intended effect is to distribute chip temperature more evenly, but the idea may be more broadly applicable, \emph{e.g.,} for heterogeneous systems.

 Some frameworks can support multiple scheduling and resource allocation policies. Guo \emph{et al.}\cite{1331730} present such a framework.  They point out that much prior work is insufficient for true QoS; merely partitioning hardware is not enough because there must also be a way to specify performance targets and an admission control policy for jobs. Unlike \pacora, they argue that targets should be expressed in terms of capacity requirements rather than rates or times.

Nesbit \emph{et al.}\cite{1436097} introduces \emph{Virtual Private Machines} (VPM), a framework for resource allocation and management in multicore systems. A VPM comprises a set of virtual hardware resources, both spatial (physical allocations) and temporal (scheduled time-slices).
%They break down the framework components into policies and mechanisms which may be implemented in hardware or software.
VPM \emph{modeling} maps high-level application objectives via
\emph{translation}, which uses models to assign acceptable VPMs to
applications while adhering to system-level policies. A
\emph{scheduler} decides if the system can accommodate all
applications. The VPM approach and terminology are similar to
\pacora's at a high level, but no design or implementation of the
modeling, translation, or scheduling components is presented~\cite{1436097}.
%mesh well with our study, which can be seen as a specific implementation of several key aspects of the type of framework they describe (i.e. VPM modeling and translation).
%
%Unlike traditional virtual machines that only virtualize resource functionality, VPMs virtualize a system's performance and power characteristics, meaning that a VPM has the same performance and power profile as a real machine with an equivalent set of hardware resources.

\section{Resource Partitioning and QoS}
\label{sec:rel:pm}

\pacora relies on resource partitioning and Quality-of-Service mechanisms when available to enforce its resource allocation decisions.  Resource partitioning and QoS research is active for on-chip, cluster, and networking resources.  However, the vast majority of research focuses on allocating a single resource type with a fixed policy, typically \emph{fairness}.
Some have researched network bandwidth fairness~\cite{Blanquer, Kleinberg99fairnessin, Liu}, while others~\cite{Baruah96proportionateprogress, Baruah_fastscheduling, Zhu} have concentrated on CPU fairness.

Hardware partitioning research, which has largely focused on caches, provides mechanisms based on policies baked into the hardware, not the flexible allocations \pacora requires \cite{876484, 967444,1194855,1275005,1194858,1318096,1088154,1399973,1069998,1399982}.  Early work focused on providing adaptive, fair policies that ensure equal performance degradation \cite{605420,1086328}, not guarantees of responsiveness. More recent proposals have incorporated more sophisticated policy management \cite{1241608,1331730,1152161,1254886}. Iyer\cite{1006246} suggests a priority-based cache QoS framework, CQoS, for shared cache way-partitioning.
%The priorities might be specified per core, per application, per memory type, or even per memory reference.
However, simultaneous achievement of performance targets as in \pacora is not addressed.


Previous work has examined how runtime measurements of application
behavior can help the operating system scheduler allocate resources.
Shen et al. showed that using hardware measurement information for
resource-aware scheduling resulted in a 15-70\% reduction in request
latency over default Linux for RUBiS, TPC-C, and
TPC-H~\cite{shen08counters}.  Another line of work investigated
techniques to co-schedule applications with disjoint resource
requirements to minimize interference, for example, executing a
compute-bound and memory-bound application
concurrently~\cite{thread_clustering,unc,shen08counters,hotos_perfcount}.
 This has
 also been done with reasonable success in~\cite{hotos_perfcount} with
 most applications receiving a 7--8\% boost in performance over
 traditional scheduling and a 58\% reduction in unfairness.  While 7\%
 seems modest, their approach uses a very simple algorithm and only few
 hardware measurement values yet still does quite well.  Few other
 hardware features could add 7\% to performance and particularly with
 such low overhead.

 Much prior work has been done to create hardware mechanisms in support of physical resource partitioning \cite{876484,967444,1194855,1086328,605420,1152161,1331730, 1241608,gsf,1250671,1194858,1275005,1088154,1318096,1399982,1399973, 1069998}.

%In general, these papers focus on designing and proving the effectiveness of particular mechanisms for particular goalswithout a concrete notion of a general-purpose resource allocation or scheduling framework.
%in which a variety of application-specific QoS requirements can be communicated to an all-purpose resource allocator and scheduler.



%The Singularity operating system\cite{aiken-mspc06} provides process isolation through software rather than hardware.  This isolation in accessibility is not the same as the performance isolation that is desirable for \pacora.

%Other partitioning work has focused on interconnect bandwidth QoS \cite{1382130} or partitioning cache capacity and bandwidth simultaneously \cite{1250671}.

%CQoS: a framework for enabling QoS in shared caches of CMP platforms
%\cite{1006246}

%Suh \emph{et al.}\cite{876484, 967444} and Qureshi and Patt \cite{1194855} monitor individual applications' cache performance to decide partitioning in an attempt to reduce the total amount of cache misses and off-chip memory traffic.
%A wide variety of proposals exist for multicore last-level cache structures that partition the spatial resources between private and shared data, in an attempt to create a manageable trade-off between capacity for shared data and low latency for private data \cite{1275005,1194858,1318096,1088154,1399973,1069998,1399982}.

%\subsection{Application Performance Modeling}
%
%\fix{add information here}


%Citations supporting PACORA's assumptions.

%Ganapathi \emph{et al.} have had success using machine learning to model application performance and select the best performing configuration in \cite{Archana}.

%Kumar \emph{et al.}\cite{1006707} demonstrate the performance advantages of heterogeneous cores for mixed workloads using heuristic allocation strategies in both space and time.
%
%Genbrugge and Eeckhout\cite{genbrugge-isca07} demonstrate the importance of adapting to changes in application characteristics, in this case instructions per cycle.

%\todo{I DON'T KNOW WHY \cite{975344,wasserman-book} ARE IN HERE}

%Scheduling threads for constructive cache sharing on CMPs
%\cite{1248396}
%
%Chen \emph{et al.}\cite{1248396} propose parallel depth-first thread scheduling as an alternative to work-stealing for constructive cache sharing.
%The paper discusses the automation of task granularity selection to match cache capacity. This kind of tuning is particularly appropriate for a user-level runtime system.


%\subsection{Autonomic Systems}
%Resource Allocation for Autonomic Data Centers using Analytic Performance Models
%	\cite{1078472}
%	
%Autonomic QoS-aware resource management in grid computing using online performance models
%	\cite{1345325}
%
%On the use of hybrid reinforcement learning for autonomic resource allocation
%\cite{1285843}
%	
%Utility-Function-Driven Resource Allocation in Autonomic Systems
%\cite{1078493}

%Redline: First Class Support for Interactivity in Commodity OSs \cite{Redline}
%Paper presented in OSDI'08.
%

%Jockey: Guaranteed Job Latency in Data Parallel Clusters \cite{Jockey}
%Work from Microsoft about the system they built on top of their Cosomos cluster (Dryad???)

%Discusses how they do the dynamic resource allocation for jobs running in a cluster
%Paper from Eurosys 2012
%


%Automatic Exploration of Datacenter Performance Regimes
%\cite{bodik-acdc09}
%

%AcOS: an Autonomic Management Layer Enhancing Commodity Operating Systems \cite{AcOS}
%Work at the Politecnico di Milano, Italy.
%Master Thesis
%Paper and slides at the CHANGE 2012 Workshop (DAC)
%
%Metronome: OS Level Performance Management via Self Adaptive Computing \cite{Metronome}
%Work related to AcOS, done by people from the Politecnico di Milano (Italy), MIT and Harvard.
%Paper presented in DAC 2012.

%A resource allocation model for QoS management
%\cite{828990}
%

%Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism to Partition Shared Caches
%\cite{1194855}
%

Many cache partitioning mechanisms previously studied have used some form of way-based partitioning \cite{1331730,1152161,605420,1250671,1194855,1086328,1399982}.  While way-based partitioning is relatively easy to implement in hardware, it is by necessity relatively coarse-grained since it is limited by the number of ways in the cache.  Partitioning among ways can cause significant performance degradation in many applications due to the decreased associativity available to each partition.

Way-based partitioning keeps the cache access logic simple since data will still always reside in the same set, regardless of current partition.  However, in the case of manycore architectures, this access simplicity will be less important since large shared caches are likely to be distributed in multiple banks.  Accessing a non-local cache bank will require a request to be sent across an interconnect. We argue that this extra communication step introduces the potential for a level of indirection in the cache indexing policy, and the added flexibility can be used to allow different banks to be assigned to different applications. This approach can be particularly advantageous in a NUCA system so that applications can be assigned cache banks nearest to their cores. Rather than sending the request to the bank identified by the address index, we can instead use the mapping of banks to applications to determine which cache bank should be the target of the incoming request.

The major overhead of this technique comes when a bank is allocated to a new application -- for the sake of the coherence policy, the system must copy back all dirty data in that bank before changing the routing used for cache requests.  We allow both bank-based and way-based partitioning of cache capacity in our implementation, though the experiments here use only bank-based partitioning to preserve cache associativity.


