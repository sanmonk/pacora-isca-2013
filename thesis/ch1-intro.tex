\chapter{Introduction}

Since the introduction of the IBM PC in 1981, client operating system kernels have been built on a 
minicomputer foundation.  The major advances since then in performance, human-computer interfaces 
and graphics, as dramatic as these have been, have left the architecture of the operating system 
relatively untouched. The list of standard OS concepts: interrupts, device drivers, priority thread 
scheduling, demand paging, and the like would be familiar to OS kernel developers of the 1980’s
Over these past thirty years, developers were able to avoid rethinking the operating system kerel 
chiefly because the hardware platform it ran on didn’t change in any fundamental way.  There was a 
CPU that ran at top speed almost all the time and was responsible for all of the instruction execution, 
and the hardware was replaced every few years by a newer edition that ran much faster. The main 
concern for the operating system kernel was what should run next on the CPU.  Memory was timemultiplexed too, to a much lesser extent, but the other resources like network and storage bandwidths 
were left unmanaged. Only the presence of specialized real-time operating systems provided a hint that 
modern, general-purpose OSes were not up to all of the tasks a developer might demand of them.
The revolution in client processor hardware that motivated the Berkeley ParLab changed the 
assumptions on which operating systems had based their modus operandi. Now, a core probably 
shouldn’t run at top speed, or all the time either, lest it waste power and run down the batter. This 
consideration applies all the more strongly when many cores are available. Worse yet, hardware 
platforms are starting to have cores of dissimilar types: big and little, CPU and GPU, smart I/O 
devices, and specialized accelerators. These heterogeneous cores can simultaneously improve system 
responsiveness and power consumption
are such decisions to be made?  
As growing on-chip hardware parallelism delivers increasing processing capabilities in personal 
computing devices, their users’ expectations grow as well. Today’s users expect snappy operation 
from high-quality multimedia and interactive applications that call for responsive user interfaces 
and stringent real-time guarantees. Moreover, the need for data security and accessibility leads to a 
number of compute-intensive applications, such as anti-virus scanners or file indexers, being executed 
in the background and possibly interfering with interactive and multimedia applications. A major 
open research challenge that ParLab sought to address with its operating system work is how an OS 
should best support this dynamically changing and complex mix of simultaneously running applications. 
Addressing this challenge means being able to satisfy quality-of-service requirements while making 
efficient use of computing resources – a complex optimization problem at best.
Both research projects in this chapter, Tessellation and PACORA, explore an idea we call Adaptive 
Resource Centric Computing (ARCC). ARCC treats resources as first-class citizens that are apportioned 
to applications to guarantee predictable performance and responsiveness. Resources are isolated
so that an application cannot degrade the performance of another by stealing resources from it. An 
ARCC system automatically adjusts resource allocations to satisfy responsiveness requirements as the 
 if they are used in the right places at the right times, but how 
environment changes, thereby reducing the burden on the user. 
The first paper, Tessellation: Refactoring the OS around Explicit Resource Containers with Continuous 
Adaptation, describes Tessellation OS, a research operating system developed at ParLab that adaptively 
partitions the resources of the system in space and time.    The Tessellation research project sought to 
answer many open research questions resulting from the transition to multicore hardware and the shift 
in user expectations such as: How should the OS provide QoS guarantees to applications without the 
need to overprovision?  How should non-traditional cores and resources be managed?  How do these 
assorted cores communicate with each other or with operating system services? Should applications be 
prevented from using too many system resources, thus preventing “denial-of-service” attacks against 
other applications?  If so, how should this goal be accomplished?
In Tessellation, software components run in containers called cells that strongly isolate the resources 
they have been allocated from the allocations and activities of the other cells. Cells are connected 
to one another via message passing channels.  The implementation of cells on a typical multicore 
platform resembles the implementation of a hypervisor with the explicit focus on resource guarantees. 
Applications are built from a series of cells, each of which may have its own QoS needs. 
In keeping with the model of ARCC, the system allocates resource quantities, partitions the physical 
resources among cells, and maps cells to hardware via a bin-packing process that attempts to utilize 
resources as efficiently as possible. Events like initiation, termination, or resource changes for 
performance reasons entail re-allocation, re-partitioning, and re-packing. The resource allocation 
process is aided through observation and modeling of software components running in cells, a process 
enabled by the stable computing environment of the cell.
In each cell, a user-mode scheduler determines how the cell’s allocated resources are used by the rest 
of the software in that cell.  These user-mode schedulers are crafted in an application-specific fashion 
on top of scheduler-frameworks exported by the Tessellation kernel. This idea has been popular in 
parallel computing for a long time but is new to mainstream client systems.  Tessellation calls it twolevel scheduling, but the number of levels can clearly be extended beyond two by recursively allocating, 
partitioning, and bin-packing to create a resource allocation hierarchy. The design requires adaptive 
resource allocation and revocation; thus all levels of the Tessellation system are designed to handle 
changing resource allocations. 
Rather than incorporating drivers directly into the OS, Tessellation wraps devices in services that also 
run in cells and communicate with their clients over channels. With the incorporation of a custom, QoSaware scheduler, these services may provide guaranteed service to other cells.  Further, Tessellation 
services may be constructed hierarchically from other services and export a variety of new functionality 
with resource guarantees. These services become allocatable resources that may be acccessed by other 
cells; consequently, the resource allocator must be capable of assigning appropriate service contracts 
to cells that implement or use services in addition to handling more primitive resources such as the 
number of cores or amount of memory.
To summarize, in the Tessellation model of computation, traditional applications are replaced by graphs 
of interconnected services, each with its own QoS requirements and guarantees. During the course of 
the project, we explored a number of different services including a Graphical User Interface (GUI) and 
a Network Service. We also build a number of applications that used these services, as described in the 
first paper. The ARCC model can easily extend to both mobile and cloud environments.
The second paper, PACORA: Optimizing Resource Allocations for Dynamic Interactive Applications, 
describes the primary resource allocation algorithm used in Tessellation.  The PACORA project sought to 
answer questions such as: How should varying needs for processing power, memory capacity, network 
bandwidth, etc. be determined and traded off among concurrently executing applications? If more 
resources would make an application run faster, exactly how fast should it run? 
PACORA is a generic resource allocation algorithm that specifically focuses on the problem of how 
much of each resource type to assign to each application. PACORA can be applied in many settings in 
addition to client operating systems such as Tessellation: it could adaptively and continuously right-size 
the multiple virtual machines in a corporate server, deliver real-time responsiveness in an embedded 
system, or adjust resources to meet SLAs in an implementation of a cloud service.
Unlike resource allocators that attempt to maximize a system’s aggregate performance or its hardware 
utilization, PACORA mathematically optimizes a single objective function designed to accurately reflect 
the value of the system to its customer(s).  This point of view is often cast in the literature as the 
problem of defining and maximizing utility; PACORA minimizes a negative utility - the penalty - instead.  
PACORA takes a different approach to resource allocation than traditional systems, relying heavily 
on application-specific functions built through measurement and convex optimization. Using convex 
optimization lets PACORA perform real-time resource allocation inexpensively, letting PACORA 
dynamically allocate resources to adjust to the changing state of the system. The optimization problems 
involved are tiny by contemporary standards, and solutions are quite fast.  Moreover, the adaptive, 
closed-loop nature of the allocation process means that a solution need not be optimal to be beneficial; 
PACORA is incessantly working to reduce total penalty.
By building application-specific functions online and formulating resource allocation as an optimization 
problem, PACORA is able to accomplish multi-dimensional resource allocation on a general set of 
resources, thereby handling heterogeneity and the growing diversity of modern hardware while 
protecting application developers from needing to understand resources. PACORA explicitly represents 
application deadlines rather than simply priorities. Knowing the deadlines lets the system make 
optimizations that are difficult in today’s systems, such as running just fast enough to make the 
deadline.  Additionally, PACORA explicitly represents system power as a competing “application”, so 
resources that can do little to reduce the total penalty are automatically powered down. 
The combination of Tessellation and PACORA creates a system that is able to understand application 
deadlines and the resources needed to meet them, quickly adapting to meet deadlines as the system 
changes. In developing our ideas, we constructed the Tessellation and PACORA prototypes from the 
ground up, providing a “clean-room” view of the embodied ideas. There is obviously more research 
and development needed to accommodate the new world of multicore hardware, but the path 
forward in operating systems is much clearer than it was when the ParLab began.  For example, we 
will be exploring hierarchical resource allocation that extends beyond one physical platform and into 
distributed environments. Statistical variations in measured performance, whether due to internal 
changes in the application or insufficient isolation of resources, can be modeled to yield confidence 
limits on response times.  Just as user runtimes allow separation of core allocation from scheduling, 
user-level memory management can ultimately do the same for memory. Interconnection networks, 
whether spanning the chip or the internet, should be engineered to allow QoS guarantees in both 
bandwidth and latency. The rational basis that ARCC provides can lead operating systems from a murky 
past based on ad hoc practices into a more well-founded and promising future. 

1. A. S. Tannenbaum. Operating Systems Design and Implementation.  Prentice-Hall, Englewood 

Cliffs, NJ (1987).

2. J. A. Winter, D. H. Albones and C. A. Shoemaker, “Scalable thread scheduling and global power 

management for heterogeneous many-core architectures”, Proc PACT ’10 (2010), Vienna.

3. W. E. Walsh, G. Tesauro, J. O. Kephart, and R. Das, “Utility functions in autonomic systems”, 

Proc. International Conference on autonomic computing (2004), pp. 70-77.

Providing responsiveness is a growing need for all types of systems,
ranging from webservers and databases running on cloud systems,
through interactive multimedia applications on mobile clients, to
emerging distributed embedded systems. Additionally, systems are
required to meet these response-time demands while improving energy
efficiency in order to reduce system power consumption and improve battery
life.

Ideally, applications should be given just enough system resources
(\emph{e.g.,} nodes, processor cores, cache slices, memory pages,
various kinds of bandwidth) to meet their performance requirements
consistently.  However, allocating resources among multiple parallel,
real-time applications while satisfying all of their
\emph{Quality-of-Service} (QoS) requirements is a complex optimization
problem, particularly as modern hardware diversifies to include a
variety of parallel architectures (\emph{e.g.,} multicore, GPUs).

Historically, general-purpose OSes often attempted to represent performance
requirements with a single value (usually called a \emph{priority})
associated with a thread of computation and adjusted within the OS by
a variety of \emph{ad-hoc} mechanisms.  OSes have also been rather
unsystematic about resource allocation and hence have not provided
useful mechanisms to implement stronger performance guarantees among
multiple competing applications, making it difficult to reason about
the expected response time of an application.
%% Krste: Not sure what this text is talking about.  Other shared
%% resources either employ independent machinery (\emph{e.g.,}  memory,
%% caches), or are deemed so abundant as to require no explicit
%% management at all (\emph{e.g.,} I/O, network bandwidth).

Priority-based approaches have no mechanism to describe deadlines or
the resources required to meet them, and so must run the highest
priority applications as fast as possible on all the resources
requested.  Consequently, interactive and real-time applications are
often run needlessly fast with significant over-provisioning---wasting energy and reducing throughput by preventing other
applications from using the resources.  Evidence of this behavior can
be found in current systems of all sizes.  Cloud computing providers
routinely utilize their clusters at only 10\% to 50\% to keep the
system responsive, despite the significant impact on infrastructure
capital costs and the additional operational costs of consuming
electricity~\cite{Barroso2009,Hennessy2011}.  In some cases, cloud
providers run only a single application on a cluster to avoid
unexpected interference.  Some mobile systems have gone so far as to
limit which applications can run in the background~\cite{iOsDev} in
order to preserve responsiveness and battery life, despite the obvious
concerns for the overall user experience.  In the embedded space,
realtime developers often use completely separate systems for each
application to ensure QoS, despite the high cost of this approach.

In this paper, we present \pacora, a resource-allocation framework designed to
provide responsiveness guarantees for a simultaneous mix of throughput,
interactive, and real-time applications in an efficient, scalable
manner.  It allows both application deadlines and the relative
importance of meeting these deadlines to be specified.  Unlike
traditional systems, \pacora considers \emph{all} resource types when
making decisions.  Using runtime measurements, application-specific
performance models are built and maintained to help determine the
resources required to meet each application's performance goals.  The
quantity of each resource to give each application is determined using
convex optimization, allowing the system to make continuous trade-offs
among application responsiveness, system performance, and energy
efficiency.

We believe \pacora is applicable to many resource-allocation
scenarios, from cloud providers determining how many resources to give
each job to avoid violating Service-Level Agreements (SLAs), through databases allocating
resources to queries, to distributed embedded systems allocating
bandwidth among devices and sensors.  In this paper we choose to study
\pacora implemented in a general-purpose operating system for client
systems, because we believe this scenario has some of the most
difficult resource allocation challenges: a constantly changing
application mix requiring low overhead and fast response times, shared
resources that create more interference among the applications, and
platforms that are too diverse to allow \emph{a priori} performance
prediction.
% applications that are more likely to be written by domain experts, thus less highly optimized.

\pacora makes resource allocation decisions in \SI{350}{\micro\second} in the worst case and often faster than \SI{50}{\micro\second} in our manycore OS implementation.  Static allocation decisions are
near optimal---only 2\% from the best possible allocation on average.
\pacora is able to dynamically allocate resources to adjust to the
changing state of the system and trade off responsiveness and
energy. By automatically generating performance models of
applications, \pacora alleviates the developer from having to
understand hardware resources and requires only a few hundred bytes of
additional storage per application.  We have found that the impact of model accuracy on the quality of resource decisions is not significant; models with errors above 20\% still produce near optimal allocations.  This effect enables modeling to be feasible in real systems with noisy applications.


Although some applications will want to adapt to available resources
as discussed in the previous section, we imagine most applications
will simply want to run as well as possible given the current platform
and the mix of other jobs running on the system.  This is the task of
the operating system scheduler, which is responsible for allocating
appropriate resources to each application.  Even if only one
application is running, a new responsibility for the OS in the
manycore era is to maximize performance and energy-efficiency of that
sole app by only allocating the appropriate resources.  For example,
allocating too many cores may cause the application to slow down, or
consume additional power with no additional performance gain.  While
an adaptive application could introspect on its own behavior, this
requires additional application developer effort, and an application
does not have the global view of what else has to run in the system.
In this section, we discuss a scheduling framework that relies only on a small set of performance counters,  which can help
operating systems allocate the appropriate resources for an
application even when the application developer is oblivious to
resource usage.

%The operating system will increasingly be required to provide spatial scheduling of applications in addition to time-multiplexing to properly handle any of these plausible scenarios.

The operating system scheduler sees a changing mix of concurrent
applications with diverse resource requirements
that might vary throughout execution. Naively dividing the machine
using ``fair'' sharing may be a poor solution: some applications may not
scale well enough to utilize a given resource while other applications
may fail to meet user-driven deadlines given too few resources.  The
scheduler's task is complicated by the fact that concurrently running
applications might interfere with each other through shared resources,
causing applications to experience unpredictable performance
degradations.  For example, if two compute-bound threads are
simultaneously scheduled on two different cores of a multicore
processor, there may be no degradation versus running each in
isolation.  However, if the two threads are memory-bandwidth
constrained, simultaneous scheduling could dramatically impair
performance.  Even more complex behaviors may occur if cores or
hardware thread contexts share functional units, caches, or TLBs.

To address the scheduling and resource allocation challenges, we have been exploring a framework that combines hardware partitioning mechanisms with application modeling to predict the optimal resource allocation for a set of applications.  The approach leverages partitioning mechanisms in hardware and the operating system, which create isolation between applications.  Isolation allows us to virtualize the performance of a machine so that given a subsection of the machine (\emph{e.g.,} 2 cores and 3 cache slices) the application will behave as if it was on a separate machine of that size.  The operating system collects sample points of the application running with different resource sets using measurement hardware and then uses them to build a predictive model of the application performance for a given set of resources.

The scheduling framework decides the best resource allocation for a set of applications by performing an optimization of an objective function involving all of the models.  The objective function represents a system--level goal such as minimize total energy or maximize throughput.  %Appendix\~\ref{details} describes the different components of this scheduling framework in more detail.

\pacora is a framework designed to determine the proper amount of each
resource type to give each application.  The purpose of \pacora is to dynamically assign resources
across multiple applications to guarantee responsiveness without
over-provisioning and to adapt allocations as the application mix
changes. For example, consider a video conference scenario where each participant requires a separate,
performance-guaranteed video stream.  New participants may join the
conference and others may leave, increasing or decreasing the number
of streams running at any given time.  Simultaneously, participants
may be collaborating through web browsers, or watching shared video
clips and web searching, while their systems run compute-intensive
background tasks such as updates, virus scans, or file indexing.
Although it may be relatively straightforward to provide
responsiveness guarantees for individual applications such as video
streams, the real challenge is to do so without reserving excessive
resources, which will compromise system utilization, power
consumption, or responsiveness of other applications.



We believe \pacora is applicable to many resource-allocation
scenarios, from cloud providers determining how many resources to give
each job to avoid violating Service-Level Agreements (SLAs), through databases allocating
resources to queries, to distributed embedded systems allocating
bandwidth among devices and sensors.  These potential applications of \pacora are discussed in greater detail in Chapter~\ref{discuss}. 


